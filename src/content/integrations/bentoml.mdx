---
title: "BentoML"
slug: "bentoml"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "6527b8abdedce731d073d037"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2024-10-09T09:56:37.030Z"
  lastUpdated: "2024-10-09T09:53:34.828Z"
  createdOn: "2023-10-12T09:13:15.091Z"
integrationType: "deployer"
logo:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/6cfafd53/66d86853b0842f70eedf9d56_bentoml.png"
shortDescription: "Seamlessly Deploy Models to Production with ZenML and BentoML"
docsUrl: "https://docs.zenml.io/stack-components/model-deployers/bentoml"
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/99b6b6cd/67064aab171e78a558352cf9_image__36_.png"
seo:
  title: "Integrate BentoML with ZenML - Deployer Integrations"
  description: "Seamlessly Deploy Models to Production with ZenML and BentoML"
  canonical: "https://www.zenml.io/integrations/bentoml"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/99b6b6cd/67064aab171e78a558352cf9_image__36_.png"
  ogTitle: "Integrate BentoML with ZenML - Deployer Integrations"
  ogDescription: "Seamlessly Deploy Models to Production with ZenML and BentoML"
---

<ul id=""><li id=""><strong id="">Streamlined Model Packaging</strong>: Effortlessly package trained models into Bentos using ZenML's built-in BentoML steps</li><li id=""><strong id="">Local Model Serving</strong>: Deploy and serve models locally for development and testing with the BentoML Model Deployer</li><li id=""><strong id="">Container-based Model Serving</strong>: ZenML’s in-built steps convert your bento into Docker images and automatically push them to your Stack’s Container Registry, from where you can deploy them anywhere.</li><li id=""><strong id="">Cloud Deployment Ready</strong>: Bentos are versioned and tracked and you can fetch them from ZenML for seamless deployment to various cloud platforms using <code id="">bentoctl</code> &nbsp;or <code id="">yatai</code> .</li><li id=""><strong id="">Standardized Deployment Workflow</strong>: Establish a consistent and reproducible model deployment process across your organization</li></ul>

‍<ul id=""><li id="">Framework-agnostic model packaging and serving</li><li id="">Supports local, cloud, and Kubernetes deployments</li><li id="">Easy-to-use Python API for defining prediction services</li><li id="">Automatic generation of OpenAPI specifications</li><li id="">Built-in monitoring and logging capabilities</li></ul>

‍<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-bash">zenml model-deployer register bentoml_deployer --flavor=bentoml
zenml stack update -d bentoml_deployer</code></pre></div>

You first need to define a BentoML Service in a [service.py](http://service.py/) file and define the logic to serve your model there. It could look like the following:

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-shell">@bentoml.service(
 &nbsp; &nbsp;name=SERVICE_NAME,
)
class MNISTService:
 &nbsp; &nbsp;def __init__(self):
 &nbsp; &nbsp; &nbsp; &nbsp;# load model
 &nbsp; &nbsp; &nbsp; &nbsp;self.model = bentoml.pytorch.load_model(MODEL_NAME)
 &nbsp; &nbsp; &nbsp; &nbsp;self.model.eval()

 &nbsp; &nbsp;@bentoml.api()
 &nbsp; &nbsp;async def predict_ndarray(
 &nbsp; &nbsp; &nbsp; &nbsp;self, 
 &nbsp; &nbsp; &nbsp; &nbsp;inp: Annotated[np.ndarray, DType("float32"), Shape((28, 28))]
 &nbsp; &nbsp;) -&gt; np.ndarray:
 &nbsp; &nbsp; &nbsp; &nbsp;inp = np.expand_dims(inp, (0, 1))
 &nbsp; &nbsp; &nbsp; &nbsp;output_tensor = await self.model(torch.tensor(inp))
 &nbsp; &nbsp; &nbsp; &nbsp;return to_numpy(output_tensor)</code></pre></div>

You can then define your pipeline as follows:

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from zenml import pipeline, step
from zenml.integrations.bentoml.steps import bento_builder_step
from zenml.integrations.bentoml.steps import bentoml_model_deployer_step


@pipeline
def bento_builder_pipeline():
 &nbsp; &nbsp;model = model_training_step()
 &nbsp; &nbsp;bento = bento_builder_step(
 &nbsp; &nbsp; &nbsp; &nbsp;model=model,
 &nbsp; &nbsp; &nbsp; &nbsp;model_name="pytorch_mnist", &nbsp;# Name of the model
 &nbsp; &nbsp; &nbsp; &nbsp;model_type="pytorch", &nbsp;# Type of the model (pytorch, tensorflow, sklearn, xgboost..)
 &nbsp; &nbsp; &nbsp; &nbsp;service="service.py:CLASS_NAME", &nbsp;# Path to the service file within zenml repo
 &nbsp; &nbsp;)
 &nbsp; &nbsp;deployed_model = bentoml_model_deployer_step(
 &nbsp; &nbsp; &nbsp; &nbsp;bento=bento,
 &nbsp; &nbsp; &nbsp; &nbsp;model_name="pytorch_mnist", &nbsp;# Name of the model
 &nbsp; &nbsp; &nbsp; &nbsp;port=3001, &nbsp;# Port to be used by the http server
 &nbsp; &nbsp; &nbsp; &nbsp;deployment_type="container" # the type of deployment, either local or container
 &nbsp; &nbsp;)</code></pre></div>This code example demonstrates how to use ZenML's BentoML integration steps within a pipeline. First, the bento_builder_step packages the trained model into a Bento bundle. Then, the bentoml_model_deployer_step deploys the Bento locally or as a container, making it available for serving predictions via an HTTP endpoint.