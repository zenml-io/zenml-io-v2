---
title: "Hugging Face"
slug: "huggingface"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "66f15a3697f4dcfa37da078a"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2025-04-13T14:23:06.098Z"
  lastUpdated: "2025-04-13T08:47:47.813Z"
  createdOn: "2024-09-23T12:08:22.432Z"
integrationType: "modeling"
logo:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/4a909b44/66f15a47230d547766de53f9_66d86795fb809547dcae5c99_huggingface-p-130x130q80.png"
shortDescription: "Accelerate NLP and Computer Vision with Hugging Face Models in ZenML Pipelines"
docsUrl: "https://docs.zenml.io/how-to/pipeline-development/training-with-gpus/accelerate-distributed-training"
githubUrl: "https://github.com/zenml-io/zenml/tree/main/examples/llm_finetuning"
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/a71646c2/66f1599e1983557cdabc4786_image__32_.png"
relatedBlogPosts:
  - "huggingface-to-sagemaker"
  - "productionalizing-nlp-models-with-zenml"
  - "embedding-huggingface-datasets-visualizations-with-zenml"
seo:
  title: "Integrate Hugging Face with ZenML - Modeling Integrations"
  description: "Accelerate NLP and Computer Vision with Hugging Face Models in ZenML Pipelines"
  canonical: "https://www.zenml.io/integrations/huggingface"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/a71646c2/66f1599e1983557cdabc4786_image__32_.png"
  ogTitle: "Integrate Hugging Face with ZenML - Modeling Integrations"
  ogDescription: "Accelerate NLP and Computer Vision with Hugging Face Models in ZenML Pipelines"
---

<ul id=""><li id=""><strong id="">Seamless Integration of Hugging Face Models<br>‍</strong>Effortlessly incorporate Hugging Face pre-trained models into ZenML pipeline steps for NLP and computer vision tasks.</li><li id=""><strong id="">Access to Extensive Model Hub<br>‍</strong>Tap into Hugging Face's vast collection of state-of-the-art models, covering a wide range of architectures and domains.</li><li id=""><strong id="">Modular Pipeline Structure<br>‍</strong>Organize your NLP workflows into distinct steps for data preparation and model training, enhancing reusability and maintainability.</li><li id=""><strong id="">Reproducible Model Tracking<br>‍</strong>Track and version Hugging Face models used in your pipelines, ensuring reproducibility and ease of collaboration.</li></ul>

‍<ul id=""><li id="">Extensive library of pre-trained models for NLP and computer vision</li><li id="">Works with the <a href="https://huggingface.co/docs/datasets/en/index" id="">Datasets</a> library for efficient data handling</li><li id="">Return any model with the <a href="https://huggingface.co/docs/transformers/en/index" id="">transformers library</a> and have it tracked natively</li><li id="">Integrated with the <a href="https://huggingface.co/docs/accelerate/en/index" id="">Accelerate</a> library to orchestrate multi-node, multi-gpu workflows</li><li id="">Support for GenAI-specific toolsets like <a href="https://github.com/zenml-io/zenml-projects/tree/main/llm-lora-finetuning" id="">PEFT and LoRA fine-tuning.</a></li></ul>

‍<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from typing import Tuple
from zenml import pipeline, step
from zenml.integrations.huggingface.steps import run_with_accelerate
from transformers import (
 &nbsp; &nbsp;AutoModelForSequenceClassification,
 &nbsp; &nbsp;AutoTokenizer,
 &nbsp; &nbsp;Trainer,
 &nbsp; &nbsp;TrainingArguments,
 &nbsp; &nbsp;DistilBertForSequenceClassification,
)
from datasets import load_dataset, Dataset

@step
def prepare_data() -&gt; Tuple[Dataset, Dataset]: &nbsp;# Return any Huggingface dataset
 &nbsp; &nbsp;dataset = load_dataset("imdb")
 &nbsp; &nbsp;tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
 &nbsp; &nbsp;def tokenize_function(examples):
 &nbsp; &nbsp; &nbsp; &nbsp;return tokenizer(examples["text"], padding="max_length", truncation=True)
 &nbsp; &nbsp;tokenized_datasets = dataset.map(tokenize_function, batched=True)
 &nbsp; &nbsp;return (
 &nbsp; &nbsp; &nbsp; &nbsp;tokenized_datasets["train"].shuffle(seed=42).select(range(1000)),
 &nbsp; &nbsp; &nbsp; &nbsp;tokenized_datasets["test"].shuffle(seed=42).select(range(100)),
 &nbsp; &nbsp;)


@run_with_accelerate(num_processes=4, multi_gpu=True) &nbsp;# &nbsp;Distribute workload with accelerate
@step(enable_cache=False) 
def train_model(
 &nbsp; &nbsp;train_dataset: Dataset, eval_dataset: Dataset
) -&gt; DistilBertForSequenceClassification:
 &nbsp; &nbsp;model = AutoModelForSequenceClassification.from_pretrained(
 &nbsp; &nbsp; &nbsp; &nbsp;"distilbert-base-uncased", num_labels=2
 &nbsp; &nbsp;)
 &nbsp; &nbsp;training_args = TrainingArguments(
 &nbsp; &nbsp; &nbsp; &nbsp;output_dir="./results",
 &nbsp; &nbsp; &nbsp; &nbsp;num_train_epochs=3,
		)
 &nbsp; &nbsp;trainer = Trainer(
 &nbsp; &nbsp; &nbsp; &nbsp;model=model,
 &nbsp; &nbsp; &nbsp; &nbsp;args=training_args,
 &nbsp; &nbsp; &nbsp; &nbsp;train_dataset=train_dataset,
 &nbsp; &nbsp; &nbsp; &nbsp;eval_dataset=eval_dataset,
 &nbsp; &nbsp;)
 &nbsp; &nbsp;trainer.train()
 &nbsp; &nbsp;return model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Return any HF model to track it

@pipeline
def fine_tuning_pipeline():
 &nbsp; &nbsp;train_dataset, eval_dataset = prepare_data()
 &nbsp; &nbsp;model = train_model(train_dataset, eval_dataset)

if __name__ == "__main__":
 &nbsp; &nbsp;fine_tuning_pipeline()</code></pre></div>

‍

‍This example demonstrates how to fine-tune a Hugging Face model within a ZenML pipeline for sentiment analysis on the IMDB dataset. The pipeline consists of two main steps:

<ol id=""><li id=""><strong id="">Data Preparation</strong>: The <code id="">prepare_data</code> step loads the IMDB dataset, tokenizes it using a DistilBERT tokenizer, and returns subsets of the train and test data.</li><li id=""><strong id="">Model Training</strong>: The <code id="">train_model</code> step initializes a DistilBERT model, sets up training arguments, and fine-tunes the model on the prepared data. It also includes evaluation and model saving functionality. This runs with the <code id="">accelerate</code> library to distribute over 4 GPUs.</li></ol>

The `fine_tuning_pipeline` function combines these steps into a cohesive workflow. This structure allows for easy modification and extension of the pipeline for different NLP tasks or models.

‍