---
title: "LLM Evaluation & Prompt Tracking Showdown: A Comprehensive Comparison of Industry Tools"
slug: "a-comprehensive-comparison-of-industry-tools"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "673b2097727b55e62c63b1f5"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2024-12-09T10:22:25.175Z"
  lastUpdated: "2024-12-09T10:22:25.175Z"
  createdOn: "2024-11-18T11:10:15.948Z"
author: "jayesh-sharma"
category: "llms"
tags:
  - "llmops"
  - "llm"
date: "2024-11-18T00:00:00.000Z"
readingTime: 8 mins
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/571c8563/673b49ae1def5845bbc26a61_llmops_eval_tools.png"
seo:
  title: "LLM Evaluation & Prompt Tracking Showdown: A Comprehensive Comparison of Industry Tools - ZenML Blog"
  description: "As Large Language Models (LLMs) revolutionize software development, the challenge of ensuring their reliable performance becomes increasingly crucial. This comprehensive guide explores the landscape of LLM evaluation, from specialized platforms like Langfuse and LangSmith to cloud provider solutions from AWS, Google Cloud, and Azure. Learn how to implement effective evaluation strategies, automate testing pipelines, and choose the right tools for your specific needs. Whether you're just starting with manual evaluations or ready to build sophisticated automated pipelines, discover how to gain confidence in your LLM applications through robust evaluation practices."
  canonical: "https://www.zenml.io/blog/a-comprehensive-comparison-of-industry-tools"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/571c8563/673b49ae1def5845bbc26a61_llmops_eval_tools.png"
  ogTitle: "LLM Evaluation & Prompt Tracking Showdown: A Comprehensive Comparison of Industry Tools - ZenML Blog"
  ogDescription: "As Large Language Models (LLMs) revolutionize software development, the challenge of ensuring their reliable performance becomes increasingly crucial. This comprehensive guide explores the landscape of LLM evaluation, from specialized platforms like Langfuse and LangSmith to cloud provider solutions from AWS, Google Cloud, and Azure. Learn how to implement effective evaluation strategies, automate testing pipelines, and choose the right tools for your specific needs. Whether you're just starting with manual evaluations or ready to build sophisticated automated pipelines, discover how to gain confidence in your LLM applications through robust evaluation practices."
---

The rise of Large Language Models (LLMs) has transformed the landscape of software development, introducing capabilities that we havenâ€™t seen beforeâ€“ from crafting human-like conversations to generating creative content and extracting insights from vast amounts of text.

But this also means that the deterministic outputs and clear-cut test cases that we have been used to, so far, wonâ€™t apply anymore since generation is now involved.

The truth is, working with LLMs feels a bit like trying to tame a smart but unpredictable power. Personally, Iâ€™ve had my moments of frustration trying to get my LLMs to follow the instructions I give, consistently. One moment it's solving complex problems with remarkable accuracy, and the next it's confidently performing actions you explicitly instructed it not to. This unpredictability creates a unique challenge: how do we ensure these powerful models are actually doing what we want them to do?

This is where LLM evaluation becomes not just important, but crucial.

## Why Evaluation Matters Now More Than Ever

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/350b7b12/673b2096727b55e62c63b11e_673b167cc6d9ae5e8a598977_Group_2013991.png" alt="A diagram showing why evaluation matters: ensuring reliability, managing risks, measuring progress, building trust." />
  <figcaption>Why evaluation matters</figcaption>
</figure>

In LLM systems, evaluation involves the following:

<ul id=""><li id=""><strong id="">Ensuring Reliability</strong>: Can your users trust the outputs your LLM-powered application produces?</li><li id=""><strong id="">Managing Risks</strong>: From hallucinations to biased responses, LLMs can go wrong in ways traditional software never could</li><li id=""><strong id="">Measuring Progress</strong>: Without proper evaluation, updating your prompts or switching models feels like shooting in the dark</li><li id=""><strong id="">Building Trust</strong>: In a world where AI safety is increasingly under scrutiny, being able to demonstrate responsible AI usage is becoming a business imperative</li></ul>

## The Current Landscape

Today, we're seeing many ways in how organizations approach LLM evaluation:

<ul id=""><li id=""><strong id="">Hyperscalers</strong> (<a href="https://aws.amazon.com/free/?trk=14a4002d-4936-4343-8211-b5a150ca592b&amp;sc_channel=ps&amp;ef_id=CjwKCAiAxKy5BhBbEiwAYiW--wPV1AvgNXjV8kOYotNZwsvtz8lArE0skzYtjB4gWWwCTteSeDdgmxoCYNwQAvD_BwE:G:s&amp;s_kwcid=AL!4422!3!453325184782!e!!g!!aws!10712784856!111477279771&amp;gclid=CjwKCAiAxKy5BhBbEiwAYiW--wPV1AvgNXjV8kOYotNZwsvtz8lArE0skzYtjB4gWWwCTteSeDdgmxoCYNwQAvD_BwE" id="">AWS</a>, <a href="https://azure.microsoft.com/en-in/pricing/purchase-options/azure-account/search?icid=free-search&amp;ef_id=_k_CjwKCAiAxKy5BhBbEiwAYiW--0FkPO5XMnTzSwVAXqKxWBWyOIJkTgV5mROmbA9kbGnQ8pYOvWfDGhoCkcgQAvD_BwE_k_&amp;OCID=AIDcmmf1elj9v5_SEM__k_CjwKCAiAxKy5BhBbEiwAYiW--0FkPO5XMnTzSwVAXqKxWBWyOIJkTgV5mROmbA9kbGnQ8pYOvWfDGhoCkcgQAvD_BwE_k_&amp;gad_source=1&amp;gclid=CjwKCAiAxKy5BhBbEiwAYiW--0FkPO5XMnTzSwVAXqKxWBWyOIJkTgV5mROmbA9kbGnQ8pYOvWfDGhoCkcgQAvD_BwE" id="">Azure</a>, <a href="https://cloud.google.com/free/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=japac-IN-all-en-dr-BKWS-all-core-trial-EXA-dr-1605216&amp;utm_content=text-ad-none-none-DEV_c-CRE_644159077394-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt+-GCP-General-core+brand-main-KWID_43700074766895889-kwd-87853815&amp;userloc_1007768-network_g&amp;utm_term=KW_gcp&amp;gad_source=1&amp;gclid=CjwKCAiAxKy5BhBbEiwAYiW--3eVMt3qFRWxdW4ZiY3DND9DzJIpkUS7I-8kAqcXqinUzhx-Sc4A2BoCudwQAvD_BwE&amp;gclsrc=aw.ds&amp;hl=en" id="">GCP</a>) have integrated evaluation in their MLOps/LLMOps platform offerings and advocate for a progressive approach: start small with manual evaluation, then gradually build automated pipelines for production.</li><li id=""><strong id="">Specialized Platforms</strong> (<a href="https://www.langchain.com/langsmith" id="">LangSmith</a>, <a href="https://langfuse.com/" id="">Langfuse</a>, <a href="https://www.evidentlyai.com/" id="">Evidently</a>, <a href="https://www.braintrust.dev/" id="">Braintrust</a> and more) offer libraries (and Dashboards) designed specifically for LLM evaluation, with their own set of metrics and templates to ease usage.</li><li id=""><strong id="">In-house Solutions</strong> where organizations build custom evaluation frameworks tailored to their specific needs. These could be as simple as some scripts you run before deploying to automated CI/CD pipelines built around your product.</li></ul>

This fragmentation in approaches and tools makes it crucial for teams to understand their options and make informed decisions about their evaluation strategy.

## Understanding LLM Evaluation

LLM evaluation, at its core, involves the following three concepts, and every tool and library offers variations of the same.

<ul id=""><li id=""><strong id="">Dataset</strong>: This is the data you run your evaluations against. Entries in the dataset that you create can have an input (which corresponds to the test prompt), and an output (if you have a ground truth to compare against). The way this Dataset is designed varies from tool to tool.</li><li id=""><strong id="">An LLM function</strong>: This is the function that takes your dataset as input and produces inferences and is typically backed by an LLM call.</li><li id=""><strong id="">Evaluators</strong>: These are a set of metrics (some tools call them scorers) that your evaluations are based on. Common scorers could be correctness, PII, and others that you can find in the section above.</li></ul>

To make things clearer, here is an example of how [LangSmith](https://www.langchain.com/langsmith) does evaluations through their SDK:

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from langsmith.evaluation import evaluate

results = evaluate(
 &nbsp; &nbsp;lambda inputs: label_text(inputs["text"]), &nbsp;# your input function
 &nbsp; &nbsp;data=dataset_name, &nbsp;# the dataset
 &nbsp; &nbsp;evaluators=[correct_label], &nbsp;# the evaluators or scorers 
 &nbsp; &nbsp;experiment_prefix="Toxic Queries",
 &nbsp; &nbsp;description="Testing the baseline system.",
)</code></pre></div>

You will notice that the `evaluate` function takes in

<ul id=""><li id="">an input function that is usually your LLM call</li><li id="">a set of evaluators, or scorers that are used to evaluate outputs</li><li id="">a dataset that contains examples to run evaluations for.</li></ul>

<aside>ğŸ’¡Check out [evaluation guide from ZenML](https://docs.zenml.io/user-guide/llmops-guide/evaluation) to learn more about the importance of evaluation and how it can improve the performance of your LLM applications, like your RAG pipeline.</aside>

â€

## Types of Evaluations

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/11bb8fc9/673b2097727b55e62c63b161_673b16e757bafc67dcc7f985_interview-jobinterview.gif" alt="__wf_reserved_inherit" />
</figure>

Before diving into what metrics are commonly used to evaluate LLMs, it helps to build a mental image of how they are grouped.

<ul id=""><li id="">Model-based evaluations: All evaluations that are backed by an LLM. These could involve checking sentiment, toxicity, correctness based on ground truth, and any other custom <a href="https://hamel.dev/blog/posts/llm-judge/" id="">â€œLLM-as-judgeâ€</a> use cases you want to build for your dataset.</li><li id="">Other traditional evaluation techniques: These involve tests like length of produced output, pattern recognition in text and other metrics based on statistics (<a href="https://en.wikipedia.org/wiki/BLEU" id="">BLEU</a>, <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)" id="">ROUGE</a>) and heuristics (Levenshtein distance and so on).</li></ul>

In general, your evaluations should have a mix of these scorers along with some human oversight at crucial points of your pipeline. Almost all tools offer these metrics as templates that you can use straight away without worrying about the evaluation logic yourself. The sections below that compare these tools will talk about the options you get from them, in detail. Some common themes are as follows:

<ul id=""><li id="">Response quality and relevance</li><li id="">Factual accuracy (correctness)</li><li id="">Bias and fairness</li><li id="">Personally Identifiable Information (PII)</li><li id="">Safety and ethical considerations</li><li id="">Performance and consistency</li></ul>

## Challenges in Evaluating LLMs

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/022a5f63/673b2096727b55e62c63b117_673b170852998a11913c99d8_Group_20210.png" alt="Challenges in evaluating LLMs: Non deterministic performance, tracking changes is hard, hard to build automated pipelines" />
  <figcaption>Diagram showing challenges in evaluating LLMs</figcaption>
</figure>

Evaluation of LLMs begins right when you start using them. Anyone who has worked with building apps with LLMs is familiar with manually running some important prompts after every change you do, to test if your outputs still make sense. This approach works well on the outset but there are some problems:

<ul id=""><li id=""><strong id="">Non-deterministic performance</strong>: Running some evals manually and testing outputs can feel like guesswork as you canâ€™t quantifiably say if things got better or worse. Maybe there are some use cases that are now performing worse than before.</li><li id=""><strong id="">Tracking changes</strong>: Manual testing only goes so far. It would help if you had a track of what changes lead to what outputs and how your performance has fared over time.</li><li id=""><strong id="">Automated pipelines are not easy to build</strong>: Even when you move to automate your evaluation process to counter the problems above, you are faced with uncertainty about how best to design your pipelines, what tools to use, and where to run them. Different providers have different approaches and thereâ€™s always a fear of getting locked into vendors.</li></ul>

## The Evolution of Evaluation Approaches

There are many tools in the industry now that enable you to start with simple manual testingâ€“ eyeballing outputs for accuracy and also help you progress to more sophisticated evaluation pipelines that can automatically assess thousands of interactions across multiple dimensions.

From the [AWS blog post](https://aws.amazon.com/blogs/machine-learning/operationalize-llm-evaluation-at-scale-using-amazon-sagemaker-clarify-and-mlops-services/) on operationalizing LLM evaluations at scale:

While the experimentation phase may involve a straightforward process, transitioning to production requires customers to automate the process and enhance the robustness of the solution. Therefore, we need to deep dive on how to automate evaluationâ€¦

The key takeaway is to design solutions that use the best tools and libraries on the market and integrate them with a pipeline that automates as much work as possible (preferably from Day 1). You should also take care to make your pipeline modular so you can switch between providers and tools without much frictionâ€” this is crucial when you are experimenting and also because this space is evolving so fast, and you might feel like switching somewhere down the line.

Now letâ€™s dive into all the available tooling options in the industry that you can leverage for your projects. We will start with the specialized LLM evaluation platforms and then see how the hyper-scalers (AWS, GCP, Azure) are handling this important part of an LLM applicationâ€™s lifecycle.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/aad46a5a/673b2097727b55e62c63b143_673b18a5ea20e2b76b43046e_image_20_43_.png" alt="Tool summary for Langfuse, LangSmith, Braintrust and Evidently including github stars, supported languages, and license" />
</figure>

## Specialized LLM Evaluation Platforms

### Langfuse

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>JS/TS, Python</td><td><a href="https://github.com/langfuse/langfuse" target="_blank" rel="noopener noreferrer"><img><span>langfuse</span><img><span></span></a></td><td>6.3k</td><td>May 2023</td><td>Open source: <a href="https://github.com/langfuse/langfuse/blob/main/LICENSE" target="_blank" rel="noopener noreferrer"><img><span>LICENSE</span><img><span></span></a></td></tr></thead><tbody></tbody>
</table>
</div></div>

From the Langfuse docs,

*Langfuse is an open-source LLM engineering platform that helps teams collaboratively debug, analyze, and iterate on their LLM applications.*

#### Key features

Langfuse offers

<ul id=""><li id=""><a href="https://langfuse.com/docs/tracing" id="">observability</a> through ingestion of logs (traces) from your LLM apps into their UI</li><li id=""><a href="https://langfuse.com/docs/prompts/get-started" id="">prompt management</a> to effectively manage and version your prompts</li><li id=""><a href="https://langfuse.com/docs/datasets/overview" id="">management of your datasets</a> to run evaluations against.</li><li id=""><a href="https://langfuse.com/docs/scores/model-based-evals" id="">automated evaluations</a> for your models with built-in and custom scorers</li></ul>

#### Evaluation methods

Langfuse supports the following evaluation methods:

<ul id=""><li id=""><a href="https://langfuse.com/docs/scores/model-based-evals" id="">Model-based Evaluation</a> (LLM-as-a-Judge): With this approach, an LLMs scores a particular session, trace, or LLM call in Langfuse based on factors such as accuracy, toxicity, or hallucinations.</li><li id=""><a href="https://langfuse.com/docs/scores/annotation" id="">Manual Annotation</a> / Data Labeling (in UI): With manual annotations, you can annotate a subset of traces and observations by hand. This allows you to collaborate with your team and add scores via the Langfuse UI.</li><li id=""><a href="https://langfuse.com/docs/scores/user-feedback" id="">User Feedback</a>: You can add explicit (thumbs up/down) and implicit (click rate, time spent on page) mechanisms to get feedback from users to serve as evaluation parameters.</li><li id=""><a href="https://langfuse.com/docs/scores/custom" id="">Custom scores</a>: you can ingest custom evaluation metrics through their SDK/API.</li></ul>

Langfuse uses *scores* as a concept that evaluates your LLM outputs. You get some built-in scores by default (for example, in the LLM-as-a-judge use case, you get prompt templates for hallucinations, toxicity and more built-in) and you can also define your own methods. Read [this page](https://langfuse.com/docs/scores/custom#how-to-add-scores) to learn how to do that.

#### Process

From what Iâ€™ve seen so far, Langfuse encourages a no-code approach to evaluation using the evaluations available in the Langfuse UI and thereâ€™s no support for running these built-in evals through other platforms like your own pipeline environments.

However, you can [build external pipelines](https://langfuse.com/docs/scores/external-evaluation-pipelines) with your custom evaluation logic (for example, using other libraries like `deepevals`) and ingest traces and your scores into Langfuse to track them centrally. This essentially means using two libraries instead of one if you wish to custom build automated pipelines for your evaluation needs, but Langfuse offers more than just evaluation so it might make sense for some of you to take this route.

#### Pricing

Langfuse is open-source and can be [run locally](https://langfuse.com/docs/deployment/local) or use the [self-hosted option](https://langfuse.com/docs/deployment/self-host) (recommended for production).

If you donâ€™t want to manage it yourself, thereâ€™s also the fully-managed platform that you can start using for Free and then pay as your usage increases. Check out the [Pricing page](https://langfuse.com/pricing) for more details.

### LangSmith

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>JS/TS, Python</td><td>Not open source. SDK available: <a href="https://github.com/langchain-ai/langsmith-sdk" target="_blank" rel="noopener noreferrer"><img><span>langsmith-sdk</span><img><span></span></a></td><td>412 (on the SDK)</td><td>May 2023</td><td>Proprietary (Freemium)</td></tr></thead><tbody></tbody>
</table>
</div></div>

From the LangSmith [docs](https://docs.smith.langchain.com/),

LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.

#### Key features

LangSmith offers a platform that allows you to:

<ul id=""><li id=""><a href="https://docs.smith.langchain.com/observability/tutorials/observability" id="">observe your LLM calls</a> as traces and runs (spans within traces): it makes it easy to log your function calls through decorators that you can put in your code, alongside many other options.</li><li id=""><a href="https://docs.smith.langchain.com/evaluation" id="">evaluate LLM outputs</a> based on datasets and a set of evaluators.</li><li id="">manage and track prompts through the use of <a href="https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-types" id="">PromptTemplates</a>.</li><li id="">run <a href="https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation" id="">online evaluations</a> (like adding user feedback to your traces, labelling of responses and more) that happen automatically and asynchronously as you ingest data (say logs from your production app).</li></ul>

#### Evaluation methods

LangSmith supports the following evaluation methods:

<ul id=""><li id=""><a href="https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge" id="">LLM-as-judge</a>: These evaluators use LLMs to score outputs. They work by defining grading rules through a prompt (either your own or through templates). They can be reference-free (for example, toxicity) or use references (for correctness)</li><li id=""><a href="https://docs.smith.langchain.com/evaluation/concepts#heuristic" id="">Heuristic</a>: These are hard-coded functions that perform computations to calculate a score. You can define them as functions and these can also be reference-free (checking for attributes like size, valid JSONs) or use a reference (like pattern matching or exact matches).</li><li id=""><a href="https://docs.smith.langchain.com/evaluation/concepts#pairwise" id="">Pairwise</a>: These evaluators pick the better of two task outputs based upon some criteria. They can either use a heuristic (a deterministic function), an LLM (with a pairwise prompt) or a human.</li><li id=""><a href="https://docs.smith.langchain.com/evaluation/concepts#human" id="">Human</a>: LangSmith makes it easy for humans to review LLM outputs and traces in the UI and give their feedback.</li></ul>

### Process

With LangSmith, you can choose from a range of off-the-shelf evaluators that span across the LLM-as-judge and statistical domains. Check [this page](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators) for more information.

The SDK offers a simple way to do this in code, which also means that you can run this in any environment you wish to deploy your apps or pipelines in. The following code shows you an example of using a QA Chain-of-thought evaluator for a question-answering use case. The `Dataset` in LangSmith comprises an â€œinputsâ€ and â€œoutputsâ€ dictionary which can be used in the context of reference-free or reference-based evaluations.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from langchain_openai import ChatOpenAI
from langchain_core.prompts.prompt import PromptTemplate
from langsmith.evaluation import LangChainStringEvaluator

eval_llm = ChatOpenAI(temperature=0.0, model="gpt-3.5-turbo")
cot_qa_evaluator = LangChainStringEvaluator("cot_qa", config={"llm": eval_llm})

evaluate(
 &nbsp; &nbsp;lambda input: "Hello " + input["input"], &nbsp;# input function, typically an LLM call
 &nbsp; &nbsp;data=dataset_name, &nbsp;# the dataset to run evals for
 &nbsp; &nbsp;evaluators=[cot_qa_evaluator], &nbsp;# the built-in evaluator
)</code></pre></div>

You can also define custom evaluators and use them in your evaluation code. [Check this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators) that shows you how you can define functions and then use them in the `evaluators` parameter of the `evaluate` function.

### Building an automated pipeline

You can use the `langsmith` library with ZenML to build an automated pipeline that you can trigger on schedule or when certain events happen, for example. Check out [this section of the blog post](https://www.notion.so/LLM-Evaluation-Prompt-Tracking-Showdown-A-Comprehensive-Comparison-of-Industry-Tools-135f8dff2538805c9ce8ceea2ac52882?pvs=21) to learn how a pipeline and a LangSmith evaluation step might look like.

### Pricing

LangSmith is not open-source but you can start for free and pay as your usage scales. They have plans for hobbyists, startups and enterprises (which includes the [self-hosted](https://docs.smith.langchain.com/self_hosting) version).

Check out the [Pricing page](https://docs.smith.langchain.com/administration/pricing) for more details.

### Braintrust

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>JS/TS, Python, Java, Go, Kotlin, Ruby</td><td>Not open source. SDK available: <a href="https://github.com/braintrustdata/braintrust-sdk" target="_blank" rel="noopener noreferrer"><img><span>braintrust-sdk</span><img><span></span></a> Evals library: <a href="https://github.com/braintrustdata/autoevals" target="_blank" rel="noopener noreferrer"><img><span>autoevals</span><img><span></span></a></td><td>203 (for autoevals)</td><td>July 2023</td><td>Proprietary (Freemium)</td></tr></thead><tbody></tbody>
</table>
</div></div>

From the Braintrust [docs](https://braintrust.dev/docs/),

Braintrust is an end-to-end platform for building AI applications. It makes software development with large language models (LLMs) robust and iterative.

#### Key features

Braintrust offers an end-to-end platform that allows you to

<ul id=""><li id=""><a href="https://www.braintrust.dev/docs/guides/logging" id="">log</a>, monitor and take actions on your LLM interactions. The logs are made available as traces and spans within traces.</li><li id="">prototype with different models and prompts in their <a href="https://www.braintrust.dev/docs/guides/playground" id="">playground</a>.</li><li id=""><a href="https://www.braintrust.dev/docs/guides/evals" id="">evaluate</a> how models and prompts are performing in production.</li><li id=""><a href="https://www.braintrust.dev/docs/guides/datasets" id="">manage</a> and <a href="https://www.braintrust.dev/docs/guides/human-review" id="">review</a> data coming in from sources like your execution environmentsâ€™ logs, your custom data in files or from user feedback.</li><li id="">manage prompts, datasets, tools and custom scorers from the Braintrust UI.</li><li id="">run <a href="https://www.braintrust.dev/docs/guides/logging#online-evaluation" id="">online evaluations</a> that happen automatically and asynchronously as you ingest data (say logs from your production app).</li><li id="">use a <a href="https://github.com/braintrustdata/braintrust-proxy" id="">single API</a> to access ~100 proprietary and open-source models with features like caching (to save costs) and monitoring through Braintrust. This is possible through the <a href="https://www.braintrust.dev/docs/guides/proxy#ai-proxy" id="">Braintrust AI Proxy</a>.</li><li id="">experiment with <a href="https://www.braintrust.dev/docs/guides/functions/tools" id="">tool calling</a> with supported models easily by defining and pushing tools to Braintrust, which can then make them available to the LLM, perform the function call in a sandboxed environment, and run the model again with the results.</li></ul>

#### Evaluation methods

Evaluation in Braintrust is done through [scoring functions](https://www.braintrust.dev/docs/guides/evals/write#scorers) (similar to evaluators/scorers in other tools). You can either use scorers from an open-source library called `autoevals` that the Braintrust team maintains or [write your own custom scorers](https://www.braintrust.dev/docs/guides/functions/scorers#custom-scorers) (what most people using Braintrust do). The following evaluation methods are supported:

<ul id=""><li id=""><a href="https://github.com/braintrustdata/autoevals?tab=readme-ov-file#llm-as-a-judge" id="">LLM-as-a-Judge</a>: LLM-based evaluations that include metrics like humor, factuality, moderation, security, summarization and more.</li><li id=""><a href="https://github.com/braintrustdata/autoevals?tab=readme-ov-file#heuristic" id="">Heuristic</a>: Metrics based on hard-coded functions that perform computations to calculate a score, like Levenshtein distance, Jaccard distance and more.</li><li id=""><a href="https://www.braintrust.dev/docs/guides/human-review" id="">Human Review Scoring</a>: Integration of human feedback from end-users and experts to evaluate/compare experiments.</li><li id=""><a href="https://github.com/braintrustdata/autoevals?tab=readme-ov-file#rag" id="">RAG</a>: Evaluations for RAG applications like context precision, context relevancy, answer relevance, etc.</li><li id=""><a href="https://github.com/braintrustdata/autoevals?tab=readme-ov-file#statistical" id="">Statistical</a>: Metrics based on mathematical definitions that can be applied to your data like BLEU, ROUGE and METEOR.</li></ul>

Information on more methods in details is available in the [README](https://github.com/braintrustdata/autoevals?tab=readme-ov-file#supported-evaluation-methods) of the `autoevals` library.

#### Process

Similar to LangSmith, you have an [SDK](https://github.com/braintrustdata/braintrust-sdk) here that you can use to run evaluations on your data. This means that your Braintrust code can also run in your custom execution environments like in a pipeline step. You can choose from a list of built-in scorers from the `autoevals` library or define your own. The following example shows the `Eval` function from Braintrust being used to perform evaluations on a dataset with an input and an output (reference), using a scorer from the `autoevals` library and a custom scorer.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from braintrust import Eval
 
from autoevals import Factuality
 
 
def exact_match(input, expected, output):
 &nbsp; &nbsp;return 1 if output == expected else 0
 
 
Eval(
 &nbsp; &nbsp;"Say Hi Bot", &nbsp;# Replace with your project name
 &nbsp; &nbsp;data=lambda: [
 &nbsp; &nbsp; &nbsp; &nbsp;{
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"input": "David",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"expected": "Hi David",
 &nbsp; &nbsp; &nbsp; &nbsp;},
 &nbsp; &nbsp;], &nbsp;# Replace with your eval dataset
 &nbsp; &nbsp;task=lambda input: "Hi " + input, &nbsp;# Replace with your LLM call
 &nbsp; &nbsp;scores=[Factuality, exact_match],
)</code></pre></div>

Learn more about [writing](https://www.braintrust.dev/docs/guides/evals/write) and [running](https://www.braintrust.dev/docs/guides/evals/run) evals in the Braintrust documentation.

Building an automated pipeline

You can use the `braintrust` and `autoevals` library with ZenML to build an automated pipeline that you can trigger on schedule or when certain events happen, for example. Check out [this section of the blog post](https://www.notion.so/LLM-Evaluation-Prompt-Tracking-Showdown-A-Comprehensive-Comparison-of-Industry-Tools-135f8dff2538805c9ce8ceea2ac52882?pvs=21) to learn how a pipeline and a step with Braintrust might look like.

#### Pricing

Thereâ€™s a free plan for everyone and an enterprise plan for companies. They also generously offer all features for free if you sign up with an `.edu` email or want to use it for an open source project. Learn more [here](https://www.braintrust.dev/pricing).

### Evidently

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>Python</td><td><a href="https://github.com/evidentlyai/evidently" target="_blank" rel="noopener noreferrer"><img><span>evidently</span><img><span></span></a></td><td>5.3K</td><td>November 2020</td><td>Apache 2.0, Open source (Managed option)</td></tr></thead><tbody></tbody>
</table>
</div></div>

From the Evidently docs,

Evidently helps evaluate, test, and monitor data and ML-powered systems.

#### Key features

Evidently is a library that lets you:

<ul id=""><li id=""><a href="https://docs.evidentlyai.com/user-guide/monitoring/monitoring_overview" id="">monitor inputs and outputs</a> of your AI applications be it LLM based or traditional tabular/classification-based models.</li><li id=""><a href="https://docs.evidentlyai.com/user-guide/tests-and-reports/introduction" id="">generate tests and reports</a> for your data for metrics like data quality, data drift and more.</li><li id=""><a href="https://docs.evidentlyai.com/user-guide/tracing/tracing_overview" id="">instrument</a> your AI applications to collect data and send them to the Evidently platform.</li><li id=""><a href="https://docs.evidentlyai.com/user-guide/evaluations/evals_overview" id="">run evaluation workflows</a> using a code or no-code interface.</li></ul>

#### Evaluation Methods

Evidently offers the following evaluation methods:

<ul id=""><li id="">Traditional ML metrics. If you are using LLMs for classification, information extraction, ranking (as part of RAG), you can also use traditional ML metrics like accuracy, recall, Hit Rate, NDCG.</li><li id="">Deterministic metrics like:<ul id=""><li id=""><a href="https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#text-statistics" id="">Text statistics</a>: how long is the response, how many words are in the output, and more.</li><li id=""><a href="https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#text-patterns" id="">Text patterns</a>: using regular expressions to identify patterns in text, or exact matches.</li></ul></li><li id=""><a href="https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#llm-as-a-judge" id="">LLM as a judge</a>: For checks that require metrics like hallucinations, toxicity and more, Evidently lets you create evaluation prompts to assess your dataset or use built-in templates.</li><li id=""><a href="https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#metadata-summary" id="">Metadata Summaries</a>: Your dataset in Evidently can include metadata columns like user upvotes and downvotes. You can then run summary metrics like <code id="">ColumnSummaryMetric</code> to see a distribution of your metadata (upvotes vs downvotes, for example).</li><li id="">ML-based metrics. Evidently also allows running evaluations using built-in ML models (like sentiment evaluation) or models from HuggingFace.</li><li id=""><a href="https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#semantic-similarity" id="">Using an embedding model</a>: You can evaluate semantic similarity in two bodies of texts using an embeddings model.</li></ul>

#### Process

You have an open-source `evidently` library that allows you to run evals through code, allowing you the flexibility to choose your execution environment going forward.

When using the open-source library, the process for experimentation could look something like this:

<ul id=""><li id="">You prepare an evaluation dataset to test your LLM against.</li><li id="">You run this data inputs against your LLM or your product API.</li><li id="">Then, pass the resulting dataset to Evidently for LLM evaluation.</li></ul>

Evidently decouples the LLM API call from the evaluation call unlike other tools. The reasoning behind this is that in most production use cases, it is easier to test your API separately (you may have different deployment scenarios).

There are some metrics that are available out-of-the-box for every evaluation method, and you can also define your own custom metrics. The example below shows you how to use two built-in prompt templates for an LLM-as-a-judge evaluation. Â It is recommended to pass the OpenAI API key as an environment variable before you run this code.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from evidently.report import Report
from evidently.metric_preset import TextEvals
from evidently.descriptors import *

report = Report(metrics=[
 &nbsp; &nbsp;TextEvals(column_name="response", descriptors=[
 &nbsp; &nbsp; &nbsp; &nbsp;DeclineLLMEval(),
 &nbsp; &nbsp; &nbsp; &nbsp;PIILLMEval(include_reasoning=True), 
 &nbsp; &nbsp;])
])

report.run(reference_data= None,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; current_data= assistant_logs[:10],
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; column_mapping=column_mapping)
report </code></pre></div>

The descriptors used are the `DeclineLLMEval` which checks if the response contains a denial, and the `PIILLMEval` which checks if the response contains personally identifiable information.

Learn more about running evaluations using Evidently in [this guide](https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm#id-4.-run-evaluations).

#### Building an automated pipeline

You can use the evidently library with ZenML to build an automated pipeline that you can trigger on schedule or when certain events happen, for example. Check out [this section of the blog post](https://www.notion.so/LLM-Evaluation-Prompt-Tracking-Showdown-A-Comprehensive-Comparison-of-Industry-Tools-135f8dff2538805c9ce8ceea2ac52882?pvs=21) to learn how a pipeline and an Evidently step might look like. ZenML also integrates with Evidently to offer easy data validation for your pipelines. Check out this [blog post](https://www.zenml.io/blog/10-reasons-zenml-evidently-ais-monitoring-tool) on how ZenML and Evidently work great together.

#### Pricing

Evidently is open-source and you can use make use of the library in your apps as you wish. There also exists a managed platform that is free to begin with and has offers for different organization sizes. Check out the [Pricing page](https://www.evidentlyai.com/pricing) for more information.

## Cloud Provider Solutions

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/8c77ed38/673b2097727b55e62c63b12f_673b1c92b2f9a24d4c189656_image_20_44_.png" alt="Evaluation tools by Azure, Vertex, and SageMaker with their supported languages, github stars and more" />
</figure>

### AWS (Sagemaker)

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>JS, Python, Java, Ruby, Go</td><td>Not open source. Python SDK: <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank" rel="noopener noreferrer"><img><span>sagemaker-python-sdk</span><img><span></span></a></td><td>2.1K (Python SDK)</td><td>November 2017</td><td>Proprietary (Pay-as-you-go)</td></tr></thead><tbody></tbody>
</table>
</div></div>

#### Key features

AWS offers Sagemaker as a one stop shop for everything MLOps and now (LLMOps). Sagemaker comes with a bunch of features:

<ul id=""><li id="">Model Building &amp; Training: Built-in and custom algorithms, scalable training, hyperparameter tuning, AutoML, distributed training, debugging tools.</li><li id="">Model Deployment &amp; Inference: Managed endpoints, serverless inference, model monitoring, A/B testing, multi-model endpoints, real-time and batch inference.</li><li id="">Data Management: Data labeling, feature store, data wrangling.</li><li id="">Management &amp; Collaboration: Centralized console, IAM integration, version control, experiment tracking, team collaboration.</li><li id="">Integration &amp; Support: AWS service integration, notebook instances, pre-built containers, open-source framework support.</li><li id="">Evaluation: Detect bias and explain predictions for models</li></ul>

#### Evaluation Methods

Sagemaker uses the [fmeval library](https://github.com/aws/fmeval) (short for Foundation Model Evaluation Library) that it builds and maintains as a basis for evaluations that you run on your data. It is open-source and comes built-in with a range of algorithms for metrics like toxicity, accuracy, factual knowledge, summarization and more.

Through `fmevals` , Sagemaker offers the following evaluation methods:

<ul id=""><li id="">Statistical: metrics like ROUGE can be calculated for your LLM outputs in cases like text summarization. Read more about this on the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-accuracy-evaluation.html#clarify-accuracy-evaluation-values" id="">Sagemaker docs</a>.</li><li id="">LLM based metrics like <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-factual-knowledge-evaluation.html" id="">Factuality</a> (based on some ground truth) and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-toxicity-evaluation.html" id="">Toxicity</a>.</li><li id=""><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-semantic-robustness-evaluation.html" id="">Semantic Robustness</a> which evaluates how your model outputs change as the result of small perturbations like extra whitespaces, random upper cases, etc.</li></ul>

#### Process

You can run all of the different evaluation methods in Amazon Sagemaker either directly in the Sagemaker Studio by [creating evaluation jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate-auto-ui.html) or [use the fmeval library](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate-auto-lib.html) to run your own automatic evaluations. Letâ€™s talk about the `fmevals` library case as it offers more customizations and is flexible in terms of how you use it and where you run it.

The main steps that you need to use `fmeval` are:

<ul id=""><li id="">Create a <code id="">ModelRunner</code> which can perform invocation on your LLM. <code id="">fmeval</code> provides built-in support for Amazon SageMaker Endpoints and JumpStart LLMs. You can also extend the <code id="">ModelRunner</code> interface for any LLMs hosted anywhere.</li><li id="">Use any of the supported <a href="https://github.com/aws/fmeval/tree/main/src/fmeval/eval_algorithms" id="">eval_algorithms</a>.</li></ul>

The example below shows you how to define your data and run an evaluation algorithm on it.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python"># 1. Create a DataConfig
config = DataConfig(
 &nbsp; &nbsp;dataset_name="custom_dataset",
 &nbsp; &nbsp;dataset_uri="./custom_dataset.jsonl",
 &nbsp; &nbsp;dataset_mime_type="application/jsonlines",
 &nbsp; &nbsp;model_input_location="question",
 &nbsp; &nbsp;target_output_location="answer",
)

# 2. Use an eval algorithm
from fmeval.eval_algorithms.toxicity import Toxicity, ToxicityConfig

eval_algo = Toxicity(ToxicityConfig())

# 3. Run your evaluation
eval_output = eval_algo.evaluate(model=model_runner, dataset_config=config)</code></pre></div>

<aside>ğŸ“¢One difference in FMEval compared to other libraries is that you have to run evaluations for all your algorithms one by one and thereâ€™s no function that can run evaluations for a set of algorithms in one call.</aside>

#### Building an automated pipeline

You have two options when it comes to running evaluations in an automated pipeline setup:

<ul id=""><li id="">Create an automatic model evaluation job in the Sagemaker Studio: this can be done in a no-code fashion by selecting the model in test, the dataset, the evaluators and their config all from the Studio and triggering a job. You can then reuse the job to run more evaluations.</li><li id="">Create your own pipelines (using Sagemaker Pipelines or other tools like ZenML) and add the <code id="">fmeval</code> evaluation logic as a step in this pipeline.</li></ul>

Below is an example of how an evaluation step can look like inside a Sagemaker pipeline, taken from [AWSâ€™ blog post](https://aws.amazon.com/blogs/machine-learning/operationalize-llm-evaluation-at-scale-using-amazon-sagemaker-clarify-and-mlops-services/) on operationalizing LLM evaluations at scale, and annotated with my comments.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">def evaluation(data_s3_path, endpoint_name, data_config, model_config, algorithm_config, output_data_path,):
 &nbsp; &nbsp;from fmeval.data_loaders.data_config import DataConfig
 &nbsp; &nbsp;from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner
 &nbsp; &nbsp;from fmeval.reporting.eval_output_cells import EvalOutputCell
 &nbsp; &nbsp;from fmeval.constants import MIME_TYPE_JSONLINES


		# S3 code to download your dataset
 &nbsp; &nbsp;s3 = boto3.client("s3")

 &nbsp; &nbsp;bucket, object_key = parse_s3_url(data_s3_path)
 &nbsp; &nbsp;s3.download_file(bucket, object_key, "dataset.jsonl")

		# Creating a DataConfig like the one we created before
 &nbsp; &nbsp;config = DataConfig(
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_name=data_config["dataset_name"],
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_uri="dataset.jsonl",
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_mime_type=MIME_TYPE_JSONLINES,
 &nbsp; &nbsp; &nbsp; &nbsp;model_input_location=data_config["model_input_key"],
 &nbsp; &nbsp; &nbsp; &nbsp;target_output_location=data_config["target_output_key"],
 &nbsp; &nbsp;)
 &nbsp; &nbsp;
		# Defining an evaluation config based on the YAML config file passed to the pipeline
 &nbsp; &nbsp;evaluation_config = model_config["evaluation_config"]

 &nbsp; &nbsp;# Defining your input and inference parameters
 &nbsp; &nbsp;content_dict = {
 &nbsp; &nbsp; &nbsp; &nbsp;"inputs": evaluation_config["content_template"],
 &nbsp; &nbsp; &nbsp; &nbsp;"parameters": evaluation_config["inference_parameters"],
 &nbsp; &nbsp;}
 &nbsp; &nbsp;serializer = JSONSerializer()
 &nbsp; &nbsp;serialized_data = serializer.serialize(content_dict)

 &nbsp; &nbsp;content_template = serialized_data.replace('"PROMPT_PLACEHOLDER"', "$prompt")
 &nbsp; &nbsp;print(content_template)

 &nbsp; &nbsp;# Defining a ModelRunner that the evaluations will be run against
 &nbsp; &nbsp;js_model_runner = JumpStartModelRunner(
 &nbsp; &nbsp; &nbsp; &nbsp;endpoint_name=endpoint_name,
 &nbsp; &nbsp; &nbsp; &nbsp;model_id=model_config["model_id"],
 &nbsp; &nbsp; &nbsp; &nbsp;model_version=model_config["model_version"],
 &nbsp; &nbsp; &nbsp; &nbsp;output=evaluation_config["output"],
 &nbsp; &nbsp; &nbsp; &nbsp;content_template=content_template,
 &nbsp; &nbsp; &nbsp; &nbsp;custom_attributes="accept_eula=true",
 &nbsp; &nbsp;)

 &nbsp; &nbsp;eval_output_all = []
 &nbsp; &nbsp;s3 = boto3.resource("s3")
 &nbsp; &nbsp;output_bucket, output_index = parse_s3_url(output_data_path)

 &nbsp; &nbsp;# Running all algorithms passed as config to the pipeline
 &nbsp; &nbsp;for algorithm in algorithm_config:
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_name = algorithm["algorithm"]
 &nbsp; &nbsp; &nbsp; &nbsp;module = importlib.import_module(algorithm["module"])
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_class = getattr(module, algorithm_name)
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_config_class = getattr(module, algorithm["config"])
 &nbsp; &nbsp; &nbsp; &nbsp;# getting the eval algorithm class from the config name
 &nbsp; &nbsp; &nbsp; &nbsp;eval_algo = algorithm_class(algorithm_config_class(target_output_delimiter=algorithm["target_output_delimiter"]))
 &nbsp; &nbsp; &nbsp; &nbsp;# running the evaluation using the fmeval library
 &nbsp; &nbsp; &nbsp; &nbsp;eval_output = eval_algo.evaluate(model=js_model_runner, dataset_config=config, prompt_template=evaluation_config["prompt_template"], save=True,)
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;print(f"eval_output: {eval_output}")
 &nbsp; &nbsp; &nbsp; &nbsp;eval_output_all.append(eval_output)
 &nbsp; &nbsp; &nbsp; &nbsp;html = markdown.markdown(str(EvalOutputCell(eval_output[0])))
 &nbsp; &nbsp; &nbsp; &nbsp;file_index = (output_index + "/" + model_config["name"] + "_" + eval_algo.eval_name + ".html")
 &nbsp; &nbsp; &nbsp; &nbsp;# writing results back to S3
 &nbsp; &nbsp; &nbsp; &nbsp;s3_object = s3.Object(bucket_name=output_bucket, key=file_index)
 &nbsp; &nbsp; &nbsp; &nbsp;s3_object.put(Body=html)

 &nbsp; &nbsp;eval_result = {"model_config": model_config, "eval_output": eval_output_all}
 &nbsp; &nbsp;print(f"eval_result: {eval_result}")

 &nbsp; &nbsp;return eval_result</code></pre></div>

As you can see, itâ€™s a fairly complex process that involves:

<ul id=""><li id="">calls to your artifact storage option (S3 in this case) to download data and upload your evaluation results.</li><li id="">writing logic that parses your configs and translates them to objects in code.</li><li id="">a lot of configurations for the model you want to test.</li></ul>

Some of this can be made easier by using a ZenML pipeline. Look at [this section of the post](https://www.notion.so/LLM-Evaluation-Prompt-Tracking-Showdown-A-Comprehensive-Comparison-of-Industry-Tools-135f8dff2538805c9ce8ceea2ac52882?pvs=21) where I show how this step would look like with ZenML. Alternatively, also check out how other tools are used with ZenML, and maybe you can find an easier approach to take.

#### Pricing

The `fmeval` library is open-source and free to use. However, you might use it with the Sagemaker platform and that comes with a cost. Running pipelines on Sagemaker costs you money based on the instance type you use. Check out [this page](https://aws.amazon.com/sagemaker/pricing/?nc=sn&loc=4) to learn more about the available options and their pricing.

In addition to it, if you also use JumpStart and inference endpoints from Sagemaker, those come with their own costs. Check out the [AWS SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/) for more details.

### Google Cloud Platform (Vertex AI)

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>Python, Node, Java, Go</td><td>Not open source. Python SDK: <a href="https://github.com/googleapis/python-aiplatform" target="_blank" rel="noopener noreferrer"><img><span>python-aiplatform</span><img><span></span></a></td><td>636 (Python SDK)</td><td>May 2021</td><td>Proprietary (Pay-as-you-go)</td></tr></thead><tbody></tbody>
</table>
</div></div>

GCP offers Vertex AI, a fully managed solution for all your MLOps and LLMOps needs. Everything from model training, evaluation, and deployment can be done through this single service. Vertex AI also has a [GenAI evaluation service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) that provides you with metrics, and tools to run evaluations on your model outputs.

#### Key features

The Gen AI evaluation service can help you with the following tasks:

<ul id=""><li id="">Model Selection: The service simplifies choosing the best pre-trained model for your needs, comparing performance across benchmarks and your own data.</li><li id="">Parameter Optimization: Easily adjust model parameters (e.g., temperature) to fine-tune output quality and style.</li><li id="">Prompt Engineering Assistance: Get guidance on crafting effective prompts and templates to achieve desired model behavior.</li><li id="">Safe &amp; Effective Fine-tuning: Improve model performance for your specific use case while mitigating bias and unwanted outputs.</li><li id="">RAG Architecture Optimization: Select the most efficient Retrieval Augmented Generation (RAG) architecture for your application.</li><li id="">Continuous Model Improvement: The service facilitates migrating to newer, superior models as they become available, ensuring optimal performance.</li></ul>

#### Evaluation Methods

You have access to the following classses of evaluation methods:

<ul id=""><li id="">Autoraters: they use LLMs as a judge. Here, an LLM is specifically tailored to perform the tasks of evaluation and doesnâ€™t need ground truth. The judgments that the autoraters provide also come with explanations and a confidence score.</li><li id="">Computation: These methods use mathematical formulas to compute results based on common computation metrics (see the next section). There are also metric bundles available out-of-the-box for ease of use.</li><li id="">Human: Vertex AI makes it easy for humans to review outputs and submit feedback.</li></ul>

#### Process

With the Vertex AI SDK, you can define:

<ul id=""><li id="">criteria: which are single or multiple dimensions to run evaluations upon, for example, correctness, relevance and more.</li><li id="">metrics: these are scores that measure the LLM outputs against your criteria.</li></ul>

In many tools so far, youâ€™ve seen these two concepts merged into the idea of evaluators or scorers, sometimes using a separate config class to define the metric values to use. Google makes this separation explicit.

##### Metrics

When it comes to metrics, the Gen AI Evaluation Service provides two major types:

<ul id=""><li id=""><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics" id=""><strong id="">Model-based metrics</strong></a>: Use a proprietary Google model as a judge. You can measure model-based metrics <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#pointwise-pairwise" id="">pairwise or pointwise</a>:<ul id=""><li id=""><strong id="">Pointwise metrics</strong>: Let the judge model assess the candidate model's output based on the evaluation criteria.</li><li id=""><strong id="">Pairwise metrics</strong>: Let the judge model compare the responses of two models and pick the better one. This is often used when comparing a candidate model with the baseline model.</li></ul></li><li id=""><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#computation-based-metrics" id=""><strong id="">Computation-based metrics</strong></a>: These metrics are computed using mathematical formulas to compare the model's output against a ground truth or reference. Commonly used computation-based metrics include ROUGE and BLEU.</li></ul>

##### Running an evaluation

To run an evaluation, you can use the [EvalTask object](https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation#eval-task-class) from the Vertex AI SDK to first define your dataset and the metrics you want to use and then run evaluation on it using the `.evaluate()` function.

The following example shows you how to run an evaluation using built-in prompt templates and also custom metrics built using these built-in templates.

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python"># 1. Use the pre-defined model-based metrics directly
# &nbsp; &nbsp;Fluency is a criteria available for this metric
eval_task = EvalTask(
 &nbsp; &nbsp;dataset=EVAL_DATASET,
 &nbsp; &nbsp;metrics=[MetricPromptTemplateExamples.Pointwise.FLUENCY],
)

eval_result = eval_task.evaluate(
 &nbsp; &nbsp;model=MODEL,
)

# 2. Define a custom pointwise metric with two custom criteria
custom_text_quality = PointwiseMetric(
 &nbsp; &nbsp;metric="custom_text_quality",
 &nbsp; &nbsp;metric_prompt_template=PointwiseMetricPromptTemplate(
 &nbsp; &nbsp; &nbsp; &nbsp;criteria={
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"fluency": "Sentences flow smoothly and are easy to read, avoiding awkward phrasing or run-on sentences. Ideas and sentences connect logically, using transitions effectively where needed.",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"entertaining": "Short, amusing text that incorporates emojis, exclamations and questions to convey quick and spontaneous communication and diversion.",
 &nbsp; &nbsp; &nbsp; &nbsp;},
 &nbsp; &nbsp; &nbsp; &nbsp;rating_rubric={
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"1": "The response performs well on both criteria.",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"0": "The response is somewhat aligned with both criteria",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"-1": "The response falls short on both criteria",
 &nbsp; &nbsp; &nbsp; &nbsp;},
 &nbsp; &nbsp; &nbsp; &nbsp;input_variables=["prompt"],
 &nbsp; &nbsp;),
)

# Run evaluation using the custom_text_quality metric
eval_task = EvalTask(
 &nbsp; &nbsp;dataset=EVAL_DATASET,
 &nbsp; &nbsp;metrics=[custom_text_quality],
)
eval_result = eval_task.evaluate(
 &nbsp; &nbsp;model=MODEL,
)</code></pre></div>

There are other ways to define and run evaluations too. Learn more about them in the [Vertex AI GenAI evaluation service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation) docs.

#### Building an automated pipeline

Like in AWS, it is important to learn how to build an automated evaluation pipeline for production use-cases. The Gen AI Evaluation Service provides pre-built pipeline components for running evaluation and monitoring in production using Vertex AI Pipelines.

[The AutoSxS](https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval) (Automatic side-by-side) is a pairwise model-based evaluation tool that runs through the evaluation pipeline service. It comes with an autorater and can evaluate performances of models in the Vertex Model Registry in a pairwise fashion. This is a pre-built pipeline and you can configure your datasets, the tasks and criteria for you evaluation and other configuration.

You can also define your own pipelines in Vertex AI that use the `EvalTask` class to run model-based or computation-based evaluations, offering greater flexbility over the AutoSxS pipeline.

#### Pricing

The pricing for the Gen AI Evaluation Service is [here](https://cloud.google.com/vertex-ai/pricing#gen_ai_evaluation_service). It charges differently for pairwise and pointwise metrics and also for model-based and computation-based metrics, as expected.

That aside, if you run your evaluations in Vertex AI pipelines (either your own or the AutoSxS pipelines), you have to [pay for it](https://cloud.google.com/vertex-ai/pricing#pipelines) too.

In order to use the `EvalTask` object you need to host your models on the Vertex AI Model Registry which comes at a price too. Check out the [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/pricing#modelregistry) pricing page for more details.

### Azure (Azure AI Studio)

<div data-rt-embed-type="true"><style>
.table_component {
 Â  Â overflow: auto;
 Â  Â width: 100%;
}

.table_component table {
 Â  Â border: 1px solid #dededf;
 Â  Â height: 100%;
 Â  Â width: 100%;
 Â  Â table-layout: fixed;
 Â  Â border-collapse: collapse;
 Â  Â border-spacing: 1px;
 Â  Â text-align: left;
}

.table_component caption {
 Â  Â caption-side: top;
 Â  Â text-align: left;
}

.table_component th {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #eceff1;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}

.table_component td {
 Â  Â border: 1px solid #dededf;
 Â  Â background-color: #ffffff;
 Â  Â color: #000000;
 Â  Â padding: 5px;
}
</style>
<div class="responsive-table" role="region" tabindex="0">

 &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;
 &nbsp; &nbsp;<table><thead><tr><th>Supported Languages</th><th>GitHub URL</th><th>GitHub stars (as of Nov â€˜24)</th><th>Project Start Date</th><th>License</th></tr><tr><td>C#, C++, Go, Java, JavaScript, Objective-C, Python, Ruby, Swift</td><td>Not open source. Python SDK: <a href="https://github.com/Azure/azure-sdk-for-python/tree/main" target="_blank" rel="noopener noreferrer"><img><span>azure-sdk-for-python</span><img><span></span></a></td><td>4.6K (Python SDK)</td><td>November 2023</td><td>Proprietary (Pay-as-you-go)</td></tr></thead><tbody></tbody>
</table>
</div></div>

Azure offers the Azure AI Studio as a managed service that lets you train, monitor, evaluate and deploy models.

#### Key features

Azure AI Studio offers a host of features:

<ul id=""><li id="">Model catalog: A library of pre-trained AI models for different functionalities, including computer vision, natural language processing, speech services, and generative AI</li><li id="">Prompt flow: A visual graph for orchestrating prompts, with integrations for open-source frameworks</li><li id="">Azure AI Content Safety: A tool to detect offensive or inappropriate content in text and images</li><li id="">Enterprise-grade production: The ability to deploy AI innovations to Azure's managed infrastructure with continuous monitoring and governance</li><li id="">Serverless API deployments: Some models can be deployed as a serverless API with pay-as-you-go billing</li><li id="">Model comparison: The ability to compare models by task using open-source datasets</li><li id="">Model evaluation: The ability to evaluate models with your own test data</li><li id="">Prompt templates: Templates to avoid prompt injection attacks</li></ul>

#### Evaluation Methods

Azure supports two main methods for automated evaluations of generative AI applications:

<ul id=""><li id="">Traditional machine learning measurements: used to quantify the accuracy of generated outputs compared to expected answers.</li><li id="">AI-assisted evaluations: in this method, we use LLMs to judge the outputs based on metrics like quality of answers, safety, and more.</li></ul>

#### Process

For evaluation, Azure offers three paths:

<ul id=""><li id="">Playground: In this path, you can <a href="https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-prompts-playground" id="">manually evaluate</a> your application by configuring your dataset, choosing the model and the metrics in the Playground itself.</li><li id="">Evaluating â€œ<a href="https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow" id="">Flows</a>â€: Flows allow you to link together LLMs, prompts and other python tools. You can do this in the Studio or through the SDK/CLI. Evaluation of flows can also be triggered by either of those two ways and this helps you evaluate across a great number of metrics and data in an automated way.</li><li id="">Direct dataset evaluation: In this path, you submit a dataset that you have collected (say from the logs of your application) to the evaluation wizard, and run evaluations on it using the SDK/CLI or the Studio.</li></ul>

You can use built-in evaluators or create your own custom evaluators. You can find a list of all built-in evaluators on the [docs page here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#built-in-evaluators).

To run evaluations, you have two options:

<ul id=""><li id="">run evaluators on a single row of data: this can be used to test your metrics and polish your evaluator config.</li></ul>

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">import os

# Initialize Azure OpenAI Connection with your environment variables
model_config = {
 &nbsp; &nbsp;"azure_endpoint": os.environ.get("AZURE_OPENAI_ENDPOINT"),
 &nbsp; &nbsp;"api_key": os.environ.get("AZURE_OPENAI_API_KEY"),
 &nbsp; &nbsp;"azure_deployment": os.environ.get("AZURE_OPENAI_DEPLOYMENT"),
 &nbsp; &nbsp;"api_version": os.environ.get("AZURE_OPENAI_API_VERSION"),
}

from azure.ai.evaluation import RelevanceEvaluator

# Initialzing Relevance Evaluator
relevance_eval = RelevanceEvaluator(model_config)
# Running Relevance Evaluator on single input row
relevance_score = relevance_eval(
 &nbsp; &nbsp;response="The Alpine Explorer Tent is the most waterproof.",
 &nbsp; &nbsp;context="From the our product list,"
 &nbsp; &nbsp;" the alpine explorer tent is the most waterproof."
 &nbsp; &nbsp;" The Adventure Dining Table has higher weight.",
 &nbsp; &nbsp;query="Which tent is the most waterproof?",
)
print(relevance_score)</code></pre></div>

run evaluations on a dataset using evaluate()

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from azure.ai.evaluation import evaluate

result = evaluate(
 &nbsp; &nbsp;data="data.jsonl", # provide your data here
 &nbsp; &nbsp;evaluators={
 &nbsp; &nbsp; &nbsp; &nbsp;"relevance": relevance_eval,
 &nbsp; &nbsp; &nbsp; &nbsp;"answer_length": answer_length
 &nbsp; &nbsp;},
 &nbsp; &nbsp;# column mapping
 &nbsp; &nbsp;evaluator_config={
 &nbsp; &nbsp; &nbsp; &nbsp;"relevance": {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"column_mapping": {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"query": "${data.queries}"
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"ground_truth": "${data.ground_truth}"
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"response": "${outputs.response}"
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} 
 &nbsp; &nbsp; &nbsp; &nbsp;}
 &nbsp; &nbsp;}
 &nbsp; &nbsp;# Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project
 &nbsp; &nbsp;azure_ai_project = azure_ai_project,
 &nbsp; &nbsp;# Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL
 &nbsp; &nbsp;output_path="./myevalresults.json"
)</code></pre></div>

Learn more about how to set this up, what the requirements for your dataset are, and what configurations are available for the evaluators in [this guide on Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#getting-started).

#### Building an automated pipeline

You can define â€œflowsâ€ using [Prompt Flow](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow) in Azure AI Studio that represent a pipeline comprising your LLMs, prompts and other Python tools in a graph view. You can run flows from the Azure AI Studio or through the SDK/CLI and this acts like a way to automate some processes around your LLM development workflow.

To run automated evaluations, you can create evaluation flows in Prompt Flow that can be used to evaluate other flows. You can choose from a range of built-in evaluation flows and can customize them to suit your needs. Evaluation flows run after the flow they are evaluating and take their outputs as inputs. They can also take in other datasets like a ground truth dataset.

Learn more about how you can define these flows and run them in the [evaluation flow guide](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-develop-evaluation).

## Comparison Matrix

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/2c7b7d37/673b2096727b55e62c63b114_673b1f6e57bafc67dccf7f78_Comparison_20Matrix_20_1_.png" alt="Comparison matrix for LLM evaluation tools like Langfuse, LangSmith, Braintrust, Evidently, SageMaker, Vertex AI and Azure AI Studio" />
</figure>

â€

## Building an automated evaluation pipeline with ZenML

[ZenML](https://www.zenml.io) allows you to build pipelines that are composable and can help you experiment with multiple tools very easily. It also abstracts away responsibilities like managing your data artifacts, making them available across steps so you donâ€™t have to write custom S3 code, for example, in your step code.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/c410da6c/66db83503bc9d6004dd29474_66db7b81ddd2db4cb1f1458e_giphy_20_2_.webp" alt="__wf_reserved_inherit" />
</figure>

To get a taste of how ZenML would let you try out all the tools above, I have sample step code for some of them below. The idea is that you can build a pipeline, say a finetuning pipeline that loads your data, finetunes a model, and then evaluates it with the tool of your choice. This means that evaluation is just a step in this pipeline and the implementation of it can be switched based on the tool you use.

Or you can also run evaluations across all tools in the same pipeline to see how they fare against each other.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/5f34d742/673b2096727b55e62c63b10d_673b1f9f662468fa85316901_image_20_45_.png" alt="A finetuning pipeline with multi-tool evaluations, running in ZenML" />
  <figcaption>A finetuning pipeline with multi-tool evaluations, running in ZenML</figcaption>
</figure>

### Sample Pipeline

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from zenml import pipeline
from steps.prepare_data import prepare_data
from steps.finetune import finetune
from steps.load_evaluation_dataset import load_evaluation_dataset
from steps.braintrust_evaluation import braintrust_evaluation

@pipeline
def finetuning_pipeline(base_model_id:str):
 &nbsp; &nbsp;tokenized_dataset = prepare_data(base_model_id)
 &nbsp; &nbsp;model_id = finetune(base_model_id, tokenized_dataset)
 &nbsp; &nbsp;evaluation_dataset = load_evaluation_dataset()
 &nbsp; &nbsp;braintrust_evaluation(model_id, evaluation_dataset)</code></pre></div>

As you can see, defining pipelines in ZenML is as easy as adding a `pipeline` decorator to your Python function that links step functions together. In this pipeline, we first prepare our dataset to use for finetuning a model, then we run the finetuning and finally we prepare the evaluation dataset and run the evaluation based on a tool implementation. Here, we are importing the evaluation step we wrote using Braintrust.

<aside>ğŸ“Note that the evaluation dataset might also change with the tool. For example, if you the FMEval library by AWS, you would have to create a JSONL file in ZenMLâ€™s artifact store in the prepare evaluation dataset stage and then output a `dataset_path` to be used in the evaluation step code.</aside>

### Evaluation Steps

Letâ€™s now see how the evaluations would look like in a ZenML step. You will notice a common theme across these functions:

<ul id=""><li id="">The inputs to the functions are usually models you want to run evaluations for. These models are produced by the finetuning step and are made available to your evaluation step code by ZenML automatically. No more writing manual data operations code.</li><li id="">To track an artifact with ZenML, all you have to do is return that object in the step function. ZenML would then automatically track and version this artifact(s) in your artifact store.</li><li id="">You can also use the Artifact Store API to perform backend-agnostic operations on your artifact store. For example, to write a file to your store, you can just ZenMLâ€™s <code id="">fileio.open()</code> function which works across all storage backends like S3, GCS, Azure, and more!</li></ul>

#### Braintrust

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from zenml import step

@step
def braintrust_evaluation(model_id:str, dataset: Dataset):
 &nbsp; &nbsp;from braintrust import Eval
 &nbsp; &nbsp;
 &nbsp; &nbsp;from autoevals import Factuality

 &nbsp; &nbsp;def exact_match(input, expected, output):
 &nbsp; &nbsp; &nbsp; &nbsp;return 1 if output == expected else 0
 
 &nbsp; &nbsp;def generate_prediction(input: str):
 &nbsp; &nbsp; &nbsp; &nbsp;model = load_model(model_id)
 &nbsp; &nbsp; &nbsp; &nbsp;...
 
 &nbsp; &nbsp;Eval(
 &nbsp; &nbsp; &nbsp; &nbsp;"ZenML Braintrust",
 &nbsp; &nbsp; &nbsp; &nbsp;data=dataset,
 &nbsp; &nbsp; &nbsp; &nbsp;task=generate_prediction,
 &nbsp; &nbsp; &nbsp; &nbsp;scores=[Factuality, exact_match],
 &nbsp; &nbsp;)</code></pre></div>

In this code, we

<ul id=""><li id="">use the <code id="">dataset</code> and the <code id="">model_id</code> from previous steps</li><li id="">have a helper function to load and generate predictions from our finetuned model which takes one input parameter, as the <code id="">Eval</code> function expects.</li><li id="">perform evaluations on our model using the <code id="">Eval</code> function from Braintrust</li><li id="">define a custom metric and use one built-in metric from the <code id="">autoevals</code> library.</li></ul>

#### LangSmith

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from typing import Dict
from langchain_openai import ChatOpenAI
from langsmith.evaluation import LangChainStringEvaluator

from zenml import step

@step
def langsmith_evaluate(dataset_name: str, model_id: str):
 &nbsp; &nbsp;eval_llm = ChatOpenAI(temperature=0.0, model="gpt-3.5-turbo")
 &nbsp; &nbsp;cot_qa_evaluator = LangChainStringEvaluator("cot_qa", config={"llm": eval_llm})

 &nbsp; &nbsp;def generate_prediction(inputs: Dict[str, str]):
 &nbsp; &nbsp; &nbsp; &nbsp;model = load_model(model_id)
 &nbsp; &nbsp; &nbsp; &nbsp;...
 &nbsp; &nbsp;
 &nbsp; &nbsp;evaluate(
 &nbsp; &nbsp; &nbsp; &nbsp;generate_prediction,
 &nbsp; &nbsp; &nbsp; &nbsp;data=dataset_name,
 &nbsp; &nbsp; &nbsp; &nbsp;evaluators=[cot_qa_evaluator],
 &nbsp; &nbsp;)</code></pre></div>

In this code, we:

<ul id=""><li id="">get the <code id="">dataset_name</code> and the <code id="">model_id</code> from previous steps (prepare evaluation dataset and finetune model).</li><li id="">have a helper function that takes in a dictionary of inputs (as required by the <code id="">evaluate</code> function), loads the model and generate predictions.</li><li id="">use the <code id="">evaluate</code> function to run evaluations on our data and model with a set of evaluators.</li><li id="">define a chain of thought QA evaluator that uses an LLM as a judge.</li></ul>

#### AWSâ€™ FMEval

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">from zenml import step, load_artifact
from zenml.io import fileio
from zenml.types import HTMLString

@step
def sagemaker_evaluation(dataset_path: str, endpoint_name, data_config, model_config, algorithm_config):
 &nbsp; &nbsp;from fmeval.data_loaders.data_config import DataConfig
 &nbsp; &nbsp;from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner
 &nbsp; &nbsp;from fmeval.reporting.eval_output_cells import EvalOutputCell
 &nbsp; &nbsp;from fmeval.constants import MIME_TYPE_JSONLINES

 &nbsp; &nbsp;# copy the dataset from the artifact store to the local file system
 &nbsp; &nbsp;fileio.copy(dataset_path, "dataset.jsonl")

 &nbsp; &nbsp;# Creating a DataConfig
 &nbsp; &nbsp;config = DataConfig(
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_name=data_config["dataset_name"],
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_uri="dataset.jsonl",
 &nbsp; &nbsp; &nbsp; &nbsp;dataset_mime_type=MIME_TYPE_JSONLINES,
 &nbsp; &nbsp; &nbsp; &nbsp;model_input_location=data_config["model_input_key"],
 &nbsp; &nbsp; &nbsp; &nbsp;target_output_location=data_config["target_output_key"],
 &nbsp; &nbsp;)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# Defining an evaluation config based on the YAML config file passed to the pipeline
 &nbsp; &nbsp;evaluation_config = model_config["evaluation_config"]

 &nbsp; &nbsp;# Defining your input and inference parameters
 &nbsp; &nbsp;content_dict = {
 &nbsp; &nbsp; &nbsp; &nbsp;"inputs": evaluation_config["content_template"],
 &nbsp; &nbsp; &nbsp; &nbsp;"parameters": evaluation_config["inference_parameters"],
 &nbsp; &nbsp;}
 &nbsp; &nbsp;serializer = JSONSerializer()
 &nbsp; &nbsp;serialized_data = serializer.serialize(content_dict)

 &nbsp; &nbsp;content_template = serialized_data.replace('"PROMPT_PLACEHOLDER"', "$prompt")
 &nbsp; &nbsp;print(content_template)

 &nbsp; &nbsp;# Defining a ModelRunner that the evaluations will be run against
 &nbsp; &nbsp;js_model_runner = JumpStartModelRunner(
 &nbsp; &nbsp; &nbsp; &nbsp;endpoint_name=endpoint_name,
 &nbsp; &nbsp; &nbsp; &nbsp;model_id=model_config["model_id"],
 &nbsp; &nbsp; &nbsp; &nbsp;model_version=model_config["model_version"],
 &nbsp; &nbsp; &nbsp; &nbsp;output=evaluation_config["output"],
 &nbsp; &nbsp; &nbsp; &nbsp;content_template=content_template,
 &nbsp; &nbsp; &nbsp; &nbsp;custom_attributes="accept_eula=true",
 &nbsp; &nbsp;)

 &nbsp; &nbsp;eval_output_all = []
 &nbsp; &nbsp;eval_output_all_html = []

 &nbsp; &nbsp;# Running all algorithms passed as config to the pipeline
 &nbsp; &nbsp;for algorithm in algorithm_config:
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_name = algorithm["algorithm"]
 &nbsp; &nbsp; &nbsp; &nbsp;module = importlib.import_module(algorithm["module"])
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_class = getattr(module, algorithm_name)
 &nbsp; &nbsp; &nbsp; &nbsp;algorithm_config_class = getattr(module, algorithm["config"])
 &nbsp; &nbsp; &nbsp; &nbsp;# getting the eval algorithm class from the config name
 &nbsp; &nbsp; &nbsp; &nbsp;eval_algo = algorithm_class(algorithm_config_class(target_output_delimiter=algorithm["target_output_delimiter"]))
 &nbsp; &nbsp; &nbsp; &nbsp;# running the evaluation using the fmeval library
 &nbsp; &nbsp; &nbsp; &nbsp;eval_output = eval_algo.evaluate(model=js_model_runner, dataset_config=config, prompt_template=evaluation_config["prompt_template"], save=True,)
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;print(f"eval_output: {eval_output}")
 &nbsp; &nbsp; &nbsp; &nbsp;eval_output_all.append(eval_output)
 &nbsp; &nbsp; &nbsp; &nbsp;html = markdown.markdown(str(EvalOutputCell(eval_output[0])))
 &nbsp; &nbsp; &nbsp; &nbsp;html_zenml = HTMLString(html)
 &nbsp; &nbsp; &nbsp; &nbsp;eval_output_all_html.append(html_zenml)
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp;eval_result = {"model_config": model_config, "eval_output": eval_output_all, "eval_output_html": eval_output_all_html}
 &nbsp; &nbsp;print(f"eval_result: {eval_result}")

 &nbsp; &nbsp;# ZenML will autosave the eval_result to your artifact store
 &nbsp; &nbsp;return eval_result</code></pre></div>

In this code, we:

<ul id=""><li id="">copy the JSONL file containing our evaluation dataset into the current filesystem, from our artifact store.</li><li id="">create a <code id="">DataConfig</code>, an evaluation config, input parameters and choose our model through the <code id="">JumpStart</code> .</li><li id="">loop over all algorithms that we want to evaluate against and run them.</li><li id="">create <code id="">HTMLString</code> objects from the HTML string in the evaluation results. This enables ZenML to show you evaluations on the ZenML Dashboard.</li><li id="">return the results for them to be tracked and versioned automatically by ZenML.</li></ul>

#### Evidently

<div data-rt-embed-type="true"><pre><code fs-codehighlight-element="code" class="language-python">import pandas as pd
from typing import Tuple

from zenml import step
from zenml.types import HTMLString
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import TextEvals
from evidently.metrics import ColumnSummaryMetric
from evidently.descriptors import TextLength, Sentiment, IncludesWords

@step
def create_evidently_report(
 &nbsp; &nbsp;data: pd.DataFrame,
) -&gt; Tuple[HTMLString, HTMLString]:
 &nbsp; &nbsp;"""Create Evidently Report and TestSuite for LLM evaluation.
 &nbsp; &nbsp;
 &nbsp; &nbsp;Returns:
 &nbsp; &nbsp; &nbsp; &nbsp;Tuple containing HTML reports for both the metrics report and test suite
 &nbsp; &nbsp;"""
 &nbsp; &nbsp;# Define column mapping
 &nbsp; &nbsp;column_mapping = ColumnMapping(
 &nbsp; &nbsp; &nbsp; &nbsp;datetime=Config.DATETIME_COLUMN,
 &nbsp; &nbsp; &nbsp; &nbsp;text_features=[Config.TEXT_COLUMN, Config.REFERENCE_COLUMN],
 &nbsp; &nbsp; &nbsp; &nbsp;categorical_features=Config.METADATA_COLUMNS,
 &nbsp; &nbsp;)

 &nbsp; &nbsp;# Create Report
 &nbsp; &nbsp;metrics_report = Report(
 &nbsp; &nbsp; &nbsp; &nbsp;metrics=[
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TextEvals(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;column_name=Config.TEXT_COLUMN,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;descriptors=[
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Sentiment(),
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TextLength(),
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;IncludesWords(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;words_list=Config.COMPENSATION_KEYWORDS,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;display_name="Mention Compensation",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*[
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ColumnSummaryMetric(column_name=col)
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for col in Config.METADATA_COLUMNS
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
 &nbsp; &nbsp; &nbsp; &nbsp;]
 &nbsp; &nbsp;)

 &nbsp; &nbsp;# Run evaluations
 &nbsp; &nbsp;metrics_report.run(
 &nbsp; &nbsp; &nbsp; &nbsp;reference_data=None, current_data=data, column_mapping=column_mapping
 &nbsp; &nbsp;)

 &nbsp; &nbsp;# Convert reports to HTML strings for visualization
 &nbsp; &nbsp;metrics_html = HTMLString(metrics_report.show(mode="inline").data)

 &nbsp; &nbsp;return metrics_html
</code></pre></div>

In this code, we:

<ul id=""><li id="">get data from previous steps</li><li id="">define a <code id="">Report</code> object in Evidently where we specify what column we want to choose and the set of pre-built or custom descriptors (scorers) we want to run against.</li><li id="">generate evaluation results and create an <code id="">HTMLString</code> object that lets ZenML visualize these results in the Dashboard.</li></ul>

## Conclusion

LLM evaluation is not just a nice-to-have â€“ it's an essential practice given the inherent unreliability of language models. A robust evaluation framework gives you the confidence to experiment with different models and architectures, knowing that if they pass your custom evaluations, you're likely on solid ground.

The evaluation landscape consists of three main categories: custom evaluations (the most crucial but requiring significant effort), specialized platforms (like Langfuse), and cloud provider solutions (such as Amazon SageMaker FMEval). These tools differ primarily in their ease of use, cost structure, scalability, and most importantly, the actual value they bring to your evaluation process.When choosing your evaluation toolkit, the decision often comes down to your existing infrastructure and specific needs. If you're already invested in a cloud platform, their native solutions might be the most pragmatic choice. For more specialized requirements, platforms like Langsmith, Braintrust and more could be worth exploring.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/f3d15f53/673b2097727b55e62c63b12b_673b207fde85dfa7b6f878cf_youre-gonna-get-through-this-you-can-do-this.gif" alt="__wf_reserved_inherit" />
</figure>

I recommend taking a phased approach to LLM evaluation. Start with manual checks to establish baseline performance, then gradually transition to automated pipelines. This automation is inevitable and valuable â€“ it enables evaluation against larger datasets and allows for scheduled testing of your production applications. Tools with well-designed libraries and SDKs are particularly valuable here, as they enable independent evaluation runs that can be seamlessly integrated into your pipelines.We saw this in practice with the SageMaker example, where adding an evaluation step using the `fmeval` library enhanced the pipeline. This pattern can be replicated with other libraries that match your requirements. The key is ensuring your chosen solution is fast, easy to debug, and reliable. When selecting pipeline tools, prioritize those that offer flexibility in tool integration and enable rapid pipeline development without unnecessary complexity.

The big picture is clear: start small with custom evaluations, automate them as part of your pipeline, and then consider incorporating additional frameworks as needed. While this might seem like a significant investment, you won't regret building solid evaluation practices into your LLM workflows. It's an investment that pays dividends in reliability, confidence, and the ability to evolve your LLM applications with assurance.