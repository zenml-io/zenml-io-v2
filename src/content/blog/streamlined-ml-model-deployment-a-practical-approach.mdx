---
title: "Streamlined ML Model Deployment: A Practical Approach"
slug: "streamlined-ml-model-deployment-a-practical-approach"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "68024e6bfbb222544702098f"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2025-04-23T17:11:56.938Z"
  lastUpdated: "2025-04-23T15:58:33.369Z"
  createdOn: "2025-04-18T13:06:51.719Z"
author: "marwan-zaarab"
category: "mlops"
tags:
  - "deployment"
  - "model-control-plane"
  - "zenml-project"
date: "2025-04-18T00:00:00.000Z"
readingTime: 9 mins
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/2199f7e4/680261e7e7580388a7260cf3_oncoclear-project-main-image.png"
seo:
  title: "Streamlined ML Model Deployment: A Practical Approach - ZenML Blog"
  description: "OncoClear is an end-to-end MLOps solution that transforms raw diagnostic measurements into reliable cancer classification predictions. Built with ZenML's robust framework, it delivers enterprise-grade machine learning pipelines that can be deployed in both development and production environments."
  canonical: "https://www.zenml.io/blog/streamlined-ml-model-deployment-a-practical-approach"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/2199f7e4/680261e7e7580388a7260cf3_oncoclear-project-main-image.png"
  ogTitle: "Streamlined ML Model Deployment: A Practical Approach - ZenML Blog"
  ogDescription: "OncoClear is an end-to-end MLOps solution that transforms raw diagnostic measurements into reliable cancer classification predictions. Built with ZenML's robust framework, it delivers enterprise-grade machine learning pipelines that can be deployed in both development and production environments."
---

Your team has developed a breakthrough machine learning model with impressive metrics in your development environment. Stakeholders are excited about the potential impact. But now comes the truly challenging part—bridging the gap between that promising prototype and a reliable production tool that end users can trust with real-world decisions.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/3834abf8/68018698e51ebdd49585eca5_fastapi_docs.png" alt="__wf_reserved_inherit" />
  <figcaption>Figure 1: A FastAPI interface for the deployed breast cancer classifier model, showing the available endpoints for health checks, predictions, and debugging.</figcaption>
</figure>

## Why ML Deployment Is Uniquely Challenging

Data scientists who excel at creating sophisticated algorithms often find themselves navigating unfamiliar territory when it comes to deployment. These hurdles typically involve model serving, monitoring, and governance:

<ul id=""><li id=""><strong id="">Reproducibility problems</strong>: When deployment involves manual handoffs between data scientists and IT teams, subtle inconsistencies between environments lead to unexplained performance degradation when real-world data is introduced.</li><li id=""><strong id="">Versioning complexity</strong>: Without systematic tracking mechanisms, teams may unknowingly use outdated models or different departments may rely on varying versions of the same model.</li><li id=""><strong id="">Audit requirements</strong>: Regulated industries require comprehensive audit trails that can trace every prediction back to its training data, code, and validation processes—something most manual workflows can't support.</li></ul>

## Enterprise Solutions: Powerful but Complex

The industry has responded with sophisticated model deployment platforms: tools like **Seldon Core**, **BentoML**, and hyperscaler solutions like **SageMaker**, **Vertex AI**, and **Azure** **ML**. These platforms offer powerful capabilities but introduce critical trade-offs:

<ul id=""><li id=""><strong id="">Specialized Expertise Requirements:</strong> Demands dedicated DevOps/MLOps skills beyond core data science teams.</li><li id=""><strong id="">Infrastructure Complexity:</strong> Involves Kubernetes orchestration, security hardening, and networking configuration - or alternatively, hyperscaler ecosystem dependencies that <a href="https://www.bentoml.com/blog/comparison-between-vertex-ai-and-bentoml" id="">limit multi-cloud flexibility</a>.</li><li id=""><strong id="">Learning Curve and Cost Considerations:</strong> Substantial setup time for Kubernetes-based solutions contrasts with managed platforms' rapid onboarding but hidden compliance complexities and ongoing expenses.</li></ul>

For many organizations, these enterprise solutions theoretically address deployment needs but create practical obstacles that impede the iterative development cycles essential to ML's value proposition.

## Finding a Practical Middle Ground in the MLOps Spectrum

What if you could avoid most of the infrastructure burden while still gaining benefits like reproducibility, automation, and traceable model behavior? This is where frameworks like ZenML can bridge the gap by offering structured, reproducible ML workflows without requiring a full DevOps overhaul.

The key is integration with existing deployment platforms through [components that allow you to deploy models locally or in production, track services, and manage model servers via Python or CLI](https://docs.zenml.io/stacks/model-deployers)—all while maintaining consistent model tracking, versioning, and promotion logic regardless of where you deploy.

<div data-rt-embed-type="true"><style>
  .zenml-table {
    width: 100%;
    border-collapse: collapse;
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  }
  .zenml-table th {
    background-color: #6749c6;
    color: white;
    text-align: left;
    padding: 12px 16px;
    font-weight: 600;
    border: 1px solid #ddd;
  }
  .zenml-table td {
    padding: 12px 16px;
    border: 1px solid #ddd;
  }
  .zenml-table tr:nth-child(odd) {
    background-color: #f7f5fd;
  }
  .zenml-table a {
    color: #6749c6;
    text-decoration: none;
    font-weight: 500;
  }
  .zenml-feature {
    font-weight: 500;
  }
</style>

<table class="zenml-table">
  <tbody><tr>
    <th>Use Case</th>
    <th>Integration Options</th>
  </tr>
  <tr>
    <td class="zenml-feature">Deployment</td>
    <td>
      <a href="https://docs.zenml.io/stacks/model-deployers/mlflow">MLflow</a>, 
      <a href="https://docs.zenml.io/stacks/model-deployers/seldon">Seldon</a>, 
      <a href="https://docs.zenml.io/stacks/model-deployers/bentoml">BentoML</a>, 
      <a href="https://docs.zenml.io/stacks/model-deployers/databricks">Databricks</a>, ++
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Model Versioning &amp; Registry</td>
    <td>
      <a href="https://docs.zenml.io/user-guides/starter-guide/track-ml-models#P%3A3">ZenML MCP</a>, 
      <a href="https://docs.zenml.io/stacks/model-registries/mlflow">MLflow</a>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Experiment Tracking</td>
    <td>
      <a href="https://docs.zenml.io/stacks/experiment-trackers/mlflow">MLflow</a>, 
      <a href="https://docs.zenml.io/stacks/experiment-trackers/wandb">W&amp;B</a>, 
      <a href="https://docs.zenml.io/stacks/experiment-trackers/vertexai">VertexAI</a>, 
      <a href="https://docs.zenml.io/stacks/experiment-trackers/neptune">Neptune</a>, 
      <a href="https://docs.zenml.io/stacks/experiment-trackers/comet">Comet</a>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Promotion Logic</td>
    <td>
      <a href="https://docs.zenml.io/how-to/model-management-metrics/model-control-plane/promote-a-model">Custom</a> <a href="https://docs.zenml.io/how-to/model-management-metrics/model-control-plane/promote-a-model">@step</a> <a href="https://docs.zenml.io/how-to/model-management-metrics/model-control-plane/promote-a-model">logic + MCP</a>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Artifact &amp; Metadata Tracking</td>
    <td>
      <a href="https://docs.zenml.io/how-to/data-artifact-management/handle-data-artifacts/artifact-versioning">ZenML built-in</a>, plus <a href="https://docs.zenml.io/how-to/model-management-metrics/track-metrics-metadata">external loggers</a>
    </td>
  </tr>
</tbody></table></div>

This flexible integration approach allows teams to leverage existing tools and platforms while maintaining a consistent workflow structure and artifact lineage.

<div data-rt-embed-type="true"><style>
  .zenml-callout {
    border-left: 3px solid #6749c6;
    background-color: #f9f8fe;
    padding: 14px 16px;
    margin: 18px 0;
    border-radius: 0 4px 4px 0;
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  }
  .zenml-callout-title {
    color: #6749c6;
    margin-top: 0;
    margin-bottom: 10px;
    font-size: 16px;
    font-weight: 600;
  }
  .zenml-callout-text {
    margin-bottom: 10px;
    color: #444;
    line-height: 1.4;
    font-size: 14px;
  }
  .zenml-callout-text:last-child {
    margin-bottom: 0;
  }
</style>

<div class="zenml-callout">
  <h4 class="zenml-callout-title">Implementation Note</h4>
  
  <p class="zenml-callout-text">
    The approach in this project prioritizes simplicity, accessibility, and reduced infrastructure complexity. While it handles moderate traffic effectively, high-scale production environments may need additional orchestration. Organizations with strict latency needs (e.g., real-time processing) may require specialized serving platforms.
  </p>
  
  <p class="zenml-callout-text">
    The solution proposed here strikes a pragmatic middle ground — more structured than ad lib deployment scripts but less complex than full enterprise MLOps platforms. For many organizations, this balance provides the most value while minimizing operational overhead.
  </p>
</div></div>

## Model Handover to Production Service

A key decision in ML deployment is *how  *your production service accesses the specific model version that your pipeline selected. ZenML orchestrates model selection, but your serving infrastructure *doesn't *need ZenML access at runtime. This separation creates a clean boundary between environments. Here are two ways to achieve this decoupling:

#### Bake-In Approach: Embedding the Model in the Image

**Best for:** Smaller models where image size isn't a major concern and updates don't happen extremely frequently (as each model update requires a new image build).

In this approach, your deployment pipeline gets the approved model artifact as it builds the image and embeds it directly inside the Docker image. At runtime, your FastAPI service simply loads this pre-packaged model from a predefined local path (no ZenML connection required), which results in a completely self-contained deployment unit.

#### Volume-Mount Approach: Externalizing the Model

**Best for:** Large models (to keep image sizes down) or scenarios requiring frequent model updates without rebuilding the entire service image.

For larger models or scenarios requiring frequent updates, the volume-mount approach puts the model artifact on a host-accessible storage location. Your container then mounts this location at runtime and the service can access the model without embedding it in the image. This allows for model swapping without container rebuilds and works well for models too big to package in containers.

#### Choosing the Right Approach

Both approaches have the same benefits: they use ZenML for end-to-end versioning and artifact tracking during training, and maintain the same clean runtime separation from your development infrastructure. For many teams, starting with the bake-in pattern provides the simplest path to production while the volume-mount pattern is a natural evolution as model complexity grows.

Let's now look at how to implement the deployment pipeline itself using ZenML steps. The following examples illustrate the core structure, which can be adapted to fit either the bake-in or volume-mount pattern depending on your chosen strategy.

## Deployment as a Pipeline: How It Works in Practice

At the heart of this approach is a deployment pipeline powered by [automatic tracking infrastructure](https://docs.zenml.io/user-guides/starter-guide/manage-artifacts). Each deployment run is versioned, reproducible, and fully traceable with complete logs of inputs, outputs, environment variables, and pipeline context.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/b63a3c19/68024d1c9d530f34d839acde_deployment_pipeline.png" alt="__wf_reserved_inherit" />
  <figcaption>Figure 2: Deployment pipeline DAG showing the image building and container deployment steps along with their respective outputs.</figcaption>
</figure>

Let's walk through a simplified deployment pipeline that builds a Docker image and runs a FastAPI service based on a promoted model:

### Step 1: Building a Deployment Image

We define a pipeline step that packages our model, preprocessing logic, and service code into a self-contained Docker image:

<div data-rt-embed-type="true">


<style>
.zenml-code-block {
    font-family: 'JetBrains Mono', 'Fira Code', 'Roboto Mono', 'Courier New', monospace;
    font-size: 14px;
    line-height: 1.5;
    background-color: #f6f5ff;
    color: #3e3a5d;
    border-radius: 8px;
    padding: 16px;
    overflow-x: auto;
    box-shadow: 0 2px 8px rgba(101, 96, 205, 0.15);
    position: relative;
    border-left: 4px solid #6560cd;
    width: 100%;
    box-sizing: border-box;
    margin: 0;
}

.zenml-language-label {
    position: absolute;
    top: 8px;
    right: 12px;
    font-size: 12px;
    color: #6560cd;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.zenml-code-comment {
    color: #8b88ac;
    font-style: italic;
}

.zenml-code-keyword {
    color: #6560cd;
    font-weight: 600;
}

.zenml-code-string {
    color: #7c63e6;
}

.zenml-code-number {
    color: #9e73ef;
}

.zenml-code-function {
    color: #584dc2;
    font-weight: 600;
}

.zenml-code-variable {
    color: #3c3452;
    font-weight: 500;
}

.zenml-code-block pre {
    margin: 0;
    white-space: pre-wrap;
    width: 100%;
}
</style>


<div class="zenml-code-block">
    <div class="zenml-language-label">Python</div>
    <pre><span class="zenml-code-function">@step</span>(
    <span class="zenml-code-variable">enable_cache</span>=<span class="zenml-code-keyword">False</span>
)  <span class="zenml-code-comment"># Avoid caching image builds unless inputs are identical</span>
<span class="zenml-code-keyword">def</span> <span class="zenml-code-function">build_deployment_image</span>(
    <span class="zenml-code-variable">model_name</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">model_stage</span>: <span class="zenml-code-variable">str</span>,
) -&gt; <span class="zenml-code-variable">Annotated</span>[<span class="zenml-code-variable">str</span>, <span class="zenml-code-string">"image_name"</span>]:
    <span class="zenml-code-string">"""Builds a Docker image for the FastAPI deployment service."""</span>
    <span class="zenml-code-comment"># Define image name based on model name and stage</span>
    <span class="zenml-code-variable">image_name</span> = <span class="zenml-code-string">f"local-deployment-{model_name}:{model_stage}"</span>
    <span class="zenml-code-variable">docker_utils</span>.<span class="zenml-code-function">build_image</span>(
        <span class="zenml-code-variable">image_name</span>=<span class="zenml-code-variable">image_name</span>,
        <span class="zenml-code-variable">dockerfile</span>=<span class="zenml-code-string">"/app/api/Dockerfile"</span>,
        <span class="zenml-code-variable">build_context_root</span>=<span class="zenml-code-string">"/app/api"</span>,
    )
    <span class="zenml-code-keyword">return</span> <span class="zenml-code-variable">image_name</span></pre>
</div>

</div>

This step ensures the image is versioned and tied to the exact model and preprocessing artifacts used during training.

### Step 2: Running the Container

The next step spins up the containerized service:

<div data-rt-embed-type="true">


<style>
.zenml-code-block {
    font-family: 'JetBrains Mono', 'Fira Code', 'Roboto Mono', 'Courier New', monospace;
    font-size: 14px;
    line-height: 1.5;
    background-color: #f6f5ff;
    color: #3e3a5d;
    border-radius: 8px;
    padding: 16px;
    overflow-x: auto;
    box-shadow: 0 2px 8px rgba(101, 96, 205, 0.15);
    position: relative;
    border-left: 4px solid #6560cd;
    width: 100%;
    box-sizing: border-box;
    margin: 0;
}

.zenml-language-label {
    position: absolute;
    top: 8px;
    right: 12px;
    font-size: 12px;
    color: #6560cd;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.zenml-code-comment {
    color: #8b88ac;
    font-style: italic;
}

.zenml-code-keyword {
    color: #6560cd;
    font-weight: 600;
}

.zenml-code-string {
    color: #7c63e6;
}

.zenml-code-number {
    color: #9e73ef;
}

.zenml-code-function {
    color: #584dc2;
    font-weight: 600;
}

.zenml-code-variable {
    color: #3c3452;
    font-weight: 500;
}

.zenml-code-block pre {
    margin: 0;
    white-space: pre-wrap;
    width: 100%;
}
</style>


<div class="zenml-code-block">
    <div class="zenml-language-label">Python</div>
    <pre><span class="zenml-code-function">@step</span>(<span class="zenml-code-variable">enable_cache</span>=<span class="zenml-code-keyword">False</span>)  <span class="zenml-code-comment"># Avoid caching container runs</span>
<span class="zenml-code-keyword">def</span> <span class="zenml-code-function">run_deployment_container</span>(
    <span class="zenml-code-variable">zenml_server_url</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">zenml_api_key</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">model_name</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">model_stage</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">image_name</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">model_artifact_name</span>: <span class="zenml-code-variable">str</span> = <span class="zenml-code-string">"sklearn_classifier"</span>,
    <span class="zenml-code-variable">preprocess_pipeline_name</span>: <span class="zenml-code-variable">str</span> = <span class="zenml-code-string">"preprocess_pipeline"</span>,
    <span class="zenml-code-variable">host_port</span>: <span class="zenml-code-variable">int</span> = <span class="zenml-code-number">8000</span>,
    <span class="zenml-code-variable">container_port</span>: <span class="zenml-code-variable">int</span> = <span class="zenml-code-number">8000</span>,
) -&gt; <span class="zenml-code-variable">Tuple</span>[
    <span class="zenml-code-function">Annotated</span>[<span class="zenml-code-variable">str</span>, <span class="zenml-code-string">"container_id"</span>],
    <span class="zenml-code-function">Annotated</span>[<span class="zenml-code-variable">str</span>, <span class="zenml-code-string">"service_url"</span>],
]:
    <span class="zenml-code-string">"""Runs the Docker container for the model deployment service and logs deployment metadata."""</span>
    <span class="zenml-code-comment"># Define a unique container name based on model name and stage</span>
    <span class="zenml-code-variable">container_name</span> = (
        <span class="zenml-code-string">f"zenml-deployment-{model_name}-{model_stage}"</span>.lower().replace(
            <span class="zenml-code-string">"_"</span>, <span class="zenml-code-string">"-"</span>
        )
    )
    <span class="zenml-code-comment"># Run the container</span>
    <span class="zenml-code-variable">container</span> = <span class="zenml-code-variable">client</span>.<span class="zenml-code-variable">containers</span>.<span class="zenml-code-function">run</span>(
        <span class="zenml-code-variable">image</span>=<span class="zenml-code-variable">image_name</span>,
        <span class="zenml-code-variable">name</span>=<span class="zenml-code-variable">container_name</span>,
        <span class="zenml-code-variable">environment</span>=<span class="zenml-code-variable">env_vars</span>,
        <span class="zenml-code-variable">ports</span>={<span class="zenml-code-string">f"{container_port}/tcp"</span>: <span class="zenml-code-variable">host_port</span>},
        <span class="zenml-code-variable">detach</span>=<span class="zenml-code-keyword">True</span>,  <span class="zenml-code-comment"># Run in background</span>
        <span class="zenml-code-variable">restart_policy</span>={<span class="zenml-code-string">"Name"</span>: <span class="zenml-code-string">"unless-stopped"</span>},  <span class="zenml-code-comment"># Restart if it crashes</span>
    )

    <span class="zenml-code-variable">container_id</span> = <span class="zenml-code-variable">container</span>.<span class="zenml-code-variable">id</span>
    <span class="zenml-code-variable">service_url</span> = <span class="zenml-code-string">f"http://localhost:{host_port}"</span>
    
    <span class="zenml-code-comment"># Log metadata</span>
    <span class="zenml-code-function">log_metadata</span>(
        <span class="zenml-code-variable">metadata</span>={
            <span class="zenml-code-string">"deployment_info"</span>: {
                <span class="zenml-code-string">"deployed_at"</span>: <span class="zenml-code-variable">datetime</span>.<span class="zenml-code-variable">datetime</span>.<span class="zenml-code-function">now</span>().<span class="zenml-code-function">isoformat</span>()
                }
             },
        <span class="zenml-code-variable">model_name</span>=<span class="zenml-code-variable">model_name</span>,
        <span class="zenml-code-variable">model_version</span>=<span class="zenml-code-variable">model_stage</span>,
    )
    <span class="zenml-code-keyword">return</span> <span class="zenml-code-variable">container_id</span>, <span class="zenml-code-variable">service_url</span></pre>
</div>

</div>

This step logs deployment metadata (container ID, URL, timestamp) into ZenML, creating a traceable snapshot** **of what was deployed, when, and from where.

## Tying It All Together in a Deployment Pipeline

<div data-rt-embed-type="true">


<style>
.zenml-code-block {
    font-family: 'JetBrains Mono', 'Fira Code', 'Roboto Mono', 'Courier New', monospace;
    font-size: 14px;
    line-height: 1.5;
    background-color: #f6f5ff;
    color: #3e3a5d;
    border-radius: 8px;
    padding: 16px;
    overflow-x: auto;
    box-shadow: 0 2px 8px rgba(101, 96, 205, 0.15);
    position: relative;
    border-left: 4px solid #6560cd;
    width: 100%;
    box-sizing: border-box;
    margin: 0;
}

.zenml-language-label {
    position: absolute;
    top: 8px;
    right: 12px;
    font-size: 12px;
    color: #6560cd;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.zenml-code-comment {
    color: #8b88ac;
    font-style: italic;
}

.zenml-code-keyword {
    color: #6560cd;
    font-weight: 600;
}

.zenml-code-string {
    color: #7c63e6;
}

.zenml-code-number {
    color: #9e73ef;
}

.zenml-code-function {
    color: #584dc2;
    font-weight: 600;
}

.zenml-code-variable {
    color: #3c3452;
    font-weight: 500;
}

.zenml-code-block pre {
    margin: 0;
    white-space: pre-wrap;
    width: 100%;
}
</style>


<div class="zenml-code-block">
    <div class="zenml-language-label">Python</div>
    <pre><span class="zenml-code-function">@pipeline</span>(<span class="zenml-code-variable">enable_cache</span>=<span class="zenml-code-keyword">False</span>)  <span class="zenml-code-comment"># Deployment operations shouldn't be cached</span>
<span class="zenml-code-keyword">def</span> <span class="zenml-code-function">local_deployment</span>(
    <span class="zenml-code-variable">model_name</span>: <span class="zenml-code-variable">str</span>,
    <span class="zenml-code-variable">model_stage</span>: <span class="zenml-code-variable">str</span> = <span class="zenml-code-string">"production"</span>,
    <span class="zenml-code-comment"># ...</span>
):
    <span class="zenml-code-string">"""Deploy a model as a containerized FastAPI service."""</span>
    <span class="zenml-code-comment"># step 1: Build the deployment image</span>
    <span class="zenml-code-variable">image_name</span> = <span class="zenml-code-function">build_deployment_image</span>(
        <span class="zenml-code-variable">model_name</span>=<span class="zenml-code-variable">model_name</span>,
        <span class="zenml-code-variable">model_stage</span>=<span class="zenml-code-variable">model_stage</span>,
    )

    <span class="zenml-code-comment"># step 2: Deploy the container</span>
    <span class="zenml-code-variable">container_id</span>, <span class="zenml-code-variable">service_url</span> = <span class="zenml-code-function">run_deployment_container</span>(
        <span class="zenml-code-comment"># ...</span>
    )</pre>
</div>

</div>

## Implementing Model Promotion Systems

While deploying models is critical, equally important is determining which models should reach production in the first place. In traditional ML workflows, this decision often relies on informal team discussions or manual comparisons—introducing unnecessary risk for critical applications.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/90a8f08c/68024dd2857febef15c2ce1e_inference_pipeline.png" alt="__wf_reserved_inherit" />
  <figcaption>Figure 3: Inference pipeline visualization showing how prediction results feed into the ZenML Server&#039;s model registry, which maintains versioned models (SGD development version and RF production version) with their associated artifacts.</figcaption>
</figure>

An automated model promotion system applies objective criteria to determine which models reach production. A [Model Control Plane](https://docs.zenml.io/how-to/model-management-metrics/model-control-plane) can serve as a model registry, version manager, and lifecycle tracker, enabling:

<ul id=""><li id=""><strong id="">Versioning</strong>: Tracking all promoted models within a pipeline with full lineage and metadata</li><li id=""><strong id="">Promotion Logic</strong>: Enabling models to be promoted to staging, production, or custom stages based on objective criteria</li><li id=""><strong id="">Deployment Integration</strong>: Defining deployments declaratively, ensuring reproducibility and auditability</li></ul>

[Models vs. Model Control Plane](https://docs.zenml.io/user-guides/starter-guide/track-ml-models#what-is-a-zenml-model): When we refer to a "Model" in the context of a Model Control Plane, we're talking about a unifying business concept that tracks all components related to solving a particular problem — not just the technical model file (weights and parameters) produced by training algorithms. A Model Control Plane organizes pipelines, artifacts, metadata, and configurations that together represent the full lifecycle of addressing a specific use case.

Here's an example step that handles promotion by evaluating a new model's metrics against the current production baseline:

<div data-rt-embed-type="true"><div class="zenml-code-block">
  <div class="zenml-language-label">Python</div>
  <pre><span class="zenml-code-function">@step</span>
<span class="zenml-code-keyword">def</span> <span class="zenml-code-function">model_promoter</span>(<span class="zenml-code-variable">accuracy</span>: <span class="zenml-code-variable">float</span>, <span class="zenml-code-variable">stage</span>: <span class="zenml-code-string">"production"</span> = <span class="zenml-code-string">"production"</span>) -&gt; <span class="zenml-code-variable">bool</span>:
    <span class="zenml-code-string">"""Conditionally promote a model to production."""</span>
    <span class="zenml-code-variable">is_promoted</span> = <span class="zenml-code-keyword">False</span>

    <span class="zenml-code-keyword">if</span> <span class="zenml-code-variable">accuracy</span> &lt; <span class="zenml-code-number">0.8</span>:
        <span class="zenml-code-variable">logger</span>.<span class="zenml-code-function">info</span>(<span class="zenml-code-string">f"Model accuracy {accuracy*100:.2f}% is below 80%! Not promoting model."</span>)
    <span class="zenml-code-keyword">else</span>:
        <span class="zenml-code-comment"># Get the model in the current context</span>
        <span class="zenml-code-variable">current_model</span> = <span class="zenml-code-function">get_step_context</span>().<span class="zenml-code-variable">model</span>

        <span class="zenml-code-comment"># Compare with existing production model if any</span>
        <span class="zenml-code-variable">client</span> = <span class="zenml-code-function">Client</span>()
        <span class="zenml-code-keyword">try</span>:
            <span class="zenml-code-variable">stage_model</span> = <span class="zenml-code-variable">client</span>.<span class="zenml-code-function">get_model_version</span>(<span class="zenml-code-variable">current_model</span>.<span class="zenml-code-variable">name</span>, <span class="zenml-code-variable">stage</span>)
            <span class="zenml-code-variable">prod_accuracy</span> = <span class="zenml-code-variable">stage_model</span>.<span class="zenml-code-function">get_artifact</span>(<span class="zenml-code-string">"sklearn_classifier"</span>).<span class="zenml-code-variable">run_metadata</span>[<span class="zenml-code-string">"test_accuracy"</span>]

            <span class="zenml-code-keyword">if</span> <span class="zenml-code-variable">accuracy</span> &gt; <span class="zenml-code-variable">prod_accuracy</span>:
                <span class="zenml-code-comment"># If current model has better metrics, promote it</span>
                <span class="zenml-code-variable">current_model</span>.<span class="zenml-code-function">set_stage</span>(<span class="zenml-code-variable">stage</span>, <span class="zenml-code-variable">force</span>=<span class="zenml-code-keyword">True</span>)
                <span class="zenml-code-variable">is_promoted</span> = <span class="zenml-code-keyword">True</span>
        <span class="zenml-code-keyword">except</span> <span class="zenml-code-variable">KeyError</span>:
            <span class="zenml-code-comment"># If no model exists in production, promote current one</span>
            <span class="zenml-code-variable">current_model</span>.<span class="zenml-code-function">set_stage</span>(<span class="zenml-code-variable">stage</span>, <span class="zenml-code-variable">force</span>=<span class="zenml-code-keyword">True</span>)
            <span class="zenml-code-variable">is_promoted</span> = <span class="zenml-code-keyword">True</span>

    <span class="zenml-code-keyword">return</span> <span class="zenml-code-variable">is_promoted</span>
  </pre>
</div></div>

The power of this approach lies in the automatic tracking and versioning of every model and pipeline run behind the scenes. This translates informal practices ("this model seems good enough") into codified quality standards that align with organizational expectations for evidence-based decision making.

One key advantage of using a dedicated Model Control Plane over traditional model registries is the decoupling from experiment tracking systems. This means you can log and manage models even if they weren't trained via a specific pipeline—perfect for integrating pre-trained or externally produced models.

<div data-rt-embed-type="true"><style>
  .zenml-table {
    width: 100%;
    border-collapse: collapse;
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  }
  .zenml-table th {
    background-color: #6749c6;
    color: white;
    padding: 12px 16px;
    font-weight: 600;
    border: 1px solid #ddd;
  }
  .zenml-table td {
    padding: 12px 16px;
    border: 1px solid #ddd;
  }
  .zenml-table tr:nth-child(odd) {
    background-color: #f7f5fd;
  }
  .zenml-table a {
    color: #6749c6;
    text-decoration: none;
    font-weight: 500;
  }
  .zenml-feature {
    font-weight: 500;
    text-align: left;
  }
  .zenml-center {
    text-align: center;
  }
  .zenml-note {
    display: block;
    font-size: 14px;
    color: #666;
    margin-top: 5px;
    font-weight: normal;
  }
  .zenml-emoji {
    font-size: 18px;
  }
</style>

<table class="zenml-table">
  <tbody><tr>
    <th style="text-align: left;">Feature</th>
    <th style="text-align: center;">Traditional Model Registry</th>
    <th style="text-align: center;">ZenML Model Control Plane</th>
  </tr>
  <tr>
    <td class="zenml-feature">
      <a href="https://docs.zenml.io/how-to/model-management-metrics/model-control-plane/model-versions">Model versioning</a> &amp; 
      <a href="https://docs.zenml.io/how-to/model-management-metrics/model-control-plane/promote-a-model">staging</a>
    </td>
    <td class="zenml-center"><span class="zenml-emoji">✅</span></td>
    <td class="zenml-center"><span class="zenml-emoji">✅</span></td>
  </tr>
  <tr>
    <td class="zenml-feature">Requires experiment tracker</td>
    <td class="zenml-center"><span class="zenml-emoji">✅</span></td>
    <td class="zenml-center">
      <span class="zenml-emoji">❌</span>
      <span class="zenml-note">(fully decoupled)</span>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Centralized UI &amp; dashboard</td>
    <td class="zenml-center">
      <span class="zenml-emoji">❌</span>
      <span class="zenml-note">(often external)</span>
    </td>
    <td class="zenml-center">
      <span class="zenml-emoji">✅</span>
      <span class="zenml-note">built-in ZenML UI (Pro users)</span>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">Manual model logging</td>
    <td class="zenml-center"><span class="zenml-emoji">❌</span></td>
    <td class="zenml-center">
      <span class="zenml-emoji">✅</span>
      <span class="zenml-note">(supports CLI and programmatic)</span>
    </td>
  </tr>
  <tr>
    <td class="zenml-feature">
      <a href="https://docs.zenml.io/user-guides/starter-guide/track-ml-models#configuring-a-model-in-a-pipeline">Pipeline + registry integration</a>
    </td>
    <td class="zenml-center">Limited</td>
    <td class="zenml-center">
      <span class="zenml-emoji">✅</span>
      <span class="zenml-note">tightly integrated</span>
    </td>
  </tr>
</tbody></table></div>

## The API Interface: Where ML Meets Real-World Applications

Once models are deployed and promoted, they need to be accessible to end users through a reliable API. Here's how a well-designed FastAPI service ensures consistent preprocessing, proper error handling, and comprehensive logging:

<div data-rt-embed-type="true"><div class="zenml-code-block">
  <div class="zenml-language-label">Python</div>
  <pre><span class="zenml-code-function">@app.post</span>(<span class="zenml-code-string">"/predict"</span>, <span class="zenml-code-variable">response_model</span>=<span class="zenml-code-variable">PredictionResponse</span>)
<span class="zenml-code-keyword">async</span> <span class="zenml-code-keyword">def</span> <span class="zenml-code-function">predict</span>(<span class="zenml-code-variable">payload</span>: <span class="zenml-code-variable">FeaturesPayload</span>):
    <span class="zenml-code-string">"""Make predictions using the loaded model with preprocessing."""</span>
    <span class="zenml-code-variable">model</span> = <span class="zenml-code-variable">app_state</span>.<span class="zenml-code-variable">get</span>(<span class="zenml-code-string">"model"</span>)
    <span class="zenml-code-variable">preprocess_pipeline</span> = <span class="zenml-code-variable">app_state</span>.<span class="zenml-code-variable">get</span>(<span class="zenml-code-string">"preprocess_pipeline"</span>)

    <span class="zenml-code-keyword">if</span> <span class="zenml-code-variable">model</span> <span class="zenml-code-keyword">is</span> <span class="zenml-code-keyword">None</span>:
        <span class="zenml-code-variable">logger</span>.<span class="zenml-code-function">error</span>(<span class="zenml-code-string">"Model not loaded."</span>)
        <span class="zenml-code-function">raise</span> <span class="zenml-code-variable">HTTPException</span>(<span class="zenml-code-variable">status_code</span>=<span class="zenml-code-number">503</span>, <span class="zenml-code-variable">detail</span>=<span class="zenml-code-string">"Model not loaded."</span>)

    <span class="zenml-code-keyword">try</span>:
        <span class="zenml-code-comment"># Convert input and apply preprocessing if available</span>
        <span class="zenml-code-variable">data_to_predict</span> = [<span class="zenml-code-variable">payload</span>.<span class="zenml-code-variable">features</span>]
        <span class="zenml-code-keyword">if</span> <span class="zenml-code-variable">preprocess_pipeline</span> <span class="zenml-code-keyword">is not</span> <span class="zenml-code-keyword">None</span>:
            <span class="zenml-code-keyword">try</span>:
                <span class="zenml-code-variable">data_to_predict</span> = <span class="zenml-code-variable">preprocess_pipeline</span>.<span class="zenml-code-function">transform</span>(<span class="zenml-code-variable">data_to_predict</span>)
            <span class="zenml-code-keyword">except</span> <span class="zenml-code-variable">Exception</span> <span class="zenml-code-keyword">as</span> <span class="zenml-code-variable">e</span>:
                <span class="zenml-code-variable">logger</span>.<span class="zenml-code-function">warning</span>(<span class="zenml-code-string">f"Preprocessing failed: {e}. Using raw input."</span>)

        <span class="zenml-code-comment"># Make prediction</span>
        <span class="zenml-code-variable">prediction_result</span> = <span class="zenml-code-variable">model</span>.<span class="zenml-code-function">predict</span>(<span class="zenml-code-variable">data_to_predict</span>)
        <span class="zenml-code-variable">prediction_value</span> = <span class="zenml-code-variable">prediction_result</span>[<span class="zenml-code-number">0</span>] <span class="zenml-code-keyword">if</span> <span class="zenml-code-variable">isinstance</span>(<span class="zenml-code-variable">prediction_result</span>, (<span class="zenml-code-variable">list</span>, <span class="zenml-code-variable">np</span>.<span class="zenml-code-variable">ndarray</span>)) <span class="zenml-code-keyword">else</span> <span class="zenml-code-variable">prediction_result</span>

        <span class="zenml-code-comment"># Log prediction metadata</span>
        <span class="zenml-code-keyword">try</span>:
            <span class="zenml-code-variable">log_metadata</span>(
                <span class="zenml-code-variable">metadata</span>={{
                    <span class="zenml-code-string">"prediction_info"</span>: {{
                        <span class="zenml-code-string">"timestamp"</span>: <span class="zenml-code-variable">datetime</span>.<span class="zenml-code-function">now</span>().<span class="zenml-code-function">isoformat</span>(),
                        <span class="zenml-code-string">"input"</span>: <span class="zenml-code-function">convert_to_serializable</span>(<span class="zenml-code-variable">payload</span>.<span class="zenml-code-variable">features</span>),
                        <span class="zenml-code-string">"prediction"</span>: <span class="zenml-code-function">convert_to_serializable</span>(<span class="zenml-code-variable">prediction_value</span>),
                    }}
                }},
                <span class="zenml-code-variable">model_name</span>=<span class="zenml-code-variable">MODEL_NAME</span>,
                <span class="zenml-code-variable">model_version</span>=<span class="zenml-code-variable">MODEL_STAGE</span>,
            )
        <span class="zenml-code-keyword">except</span> <span class="zenml-code-variable">Exception</span> <span class="zenml-code-keyword">as</span> <span class="zenml-code-variable">log_error</span>:
            <span class="zenml-code-variable">logger</span>.<span class="zenml-code-function">warning</span>(<span class="zenml-code-string">f"Failed to log prediction metadata: {log_error}"</span>)

        <span class="zenml-code-keyword">return</span> <span class="zenml-code-variable">PredictionResponse</span>(<span class="zenml-code-variable">prediction</span>=<span class="zenml-code-function">convert_to_serializable</span>(<span class="zenml-code-variable">prediction_value</span>))

    <span class="zenml-code-keyword">except</span> <span class="zenml-code-variable">Exception</span> <span class="zenml-code-keyword">as</span> <span class="zenml-code-variable">e</span>:
        <span class="zenml-code-variable">logger</span>.<span class="zenml-code-function">error</span>(<span class="zenml-code-string">f"Prediction failed: {e}"</span>, <span class="zenml-code-variable">exc_info</span>=<span class="zenml-code-keyword">True</span>)
        <span class="zenml-code-function">raise</span> <span class="zenml-code-variable">HTTPException</span>(<span class="zenml-code-variable">status_code</span>=<span class="zenml-code-number">500</span>, <span class="zenml-code-variable">detail</span>=<span class="zenml-code-string">f"Prediction error: {str(e)}"</span>)
  </pre>
</div></div>

This endpoint showcases several critical best practices:

<ol id=""><li id=""><strong id="">Bundled preprocessing</strong>: The service automatically applies the same transformations used during training</li><li id=""><strong id="">Graceful degradation</strong>: If preprocessing fails, the service falls back to using raw input</li><li id=""><strong id="">Type conversion</strong>: Handling the conversion of ML outputs to properly serializable types</li><li id=""><strong id="">Comprehensive logging</strong>: Each prediction is logged with inputs, outputs, and metadata</li></ol>

## The Real Value Proposition

What does this technical implementation mean for business stakeholders? Consider a domain expert using a deployed ML system in practice:

When they submit data for classification or prediction, they're interacting with a system where every component—from data preprocessing to model selection to prediction logging—has been systematically verified. The model they're using has been automatically vetted against minimum performance thresholds and compared to previous versions to ensure improvement.

Perhaps most importantly, if a prediction later needs review, the system provides complete traceability from that specific prediction back through the model version, its training data, validation metrics, and development code. This comprehensive lineage builds the foundation of trust that's essential for adoption.

## Scaling Beyond Local Implementation

While we've focused on a relatively simple local deployment scenario, these principles transfer readily to more complex enterprise environments. The same patterns can be extended to target Kubernetes clusters or cloud-managed ML services. The model promotion system can incorporate additional domain-specific metrics like fairness assessments or robustness measures. The prediction API can be enhanced with explainability features.

The core insight remains consistent: treating deployment with the same systematic rigor applied to model development creates a foundation for reliable, traceable machine learning in production.

## Next Steps

The journey from promising algorithm to trusted production tool is challenging, but with systematic deployment pipelines and evidence-based promotion systems, it's a journey that more ML projects can successfully complete.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/b1318f59/68024e4ac77b41c570ffad7e_deployment_architecture.png" alt="__wf_reserved_inherit" />
  <figcaption>Figure 4: End-to-end ML lifecycle showing the connections between feature engineering, model training, and inference pipelines, all integrated with the ZenML Server that manages model versions and provides artifact lineage.</figcaption>
</figure>

## Try It Yourself!

We built this ML deployment project to showcase practical, accessible MLOps and it's available  in the [zenml-projects](https://github.com/zenml-io/zenml-projects) repo on our [GitHub](https://www.notion.so/zenml-repository-e6c414075a1c44e59ee11f90393d50be?pvs=21). The instructions to try it out are in the [README](https://github.com/zenml-io/zenml-projects/blob/main/oncoclear/README.md). We plan to add more deployment patterns going forward, so keep an eye out.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/d581bd56/68024ea071707f7b9d95980b_pipeline_overview.png" alt="__wf_reserved_inherit" />
</figure>

With ZenML, all of the pipelines are tracked in the dashboard. The pipeline overview shown in the screenshot above demonstrates how feature engineering, model training, and inference pipelines connect through the ZenML Server, with clear visibility of model versions and their promotion status. You can see how the entire ML lifecycle is automatically documented, giving you full traceability from data preparation through training to deployment.