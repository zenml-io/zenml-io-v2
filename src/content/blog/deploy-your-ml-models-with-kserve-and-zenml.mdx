---
title: "Deploy your ML models with KServe and ZenML"
slug: "deploy-your-ml-models-with-kserve-and-zenml"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "652fc9ad28c11336964175f2"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2023-10-20T11:10:36.096Z"
  lastUpdated: "2023-10-20T11:10:36.096Z"
  createdOn: "2023-10-18T12:03:57.014Z"
author: "safoine-el-khabich"
category: "zenml"
tags:
  - "deployment"
  - "integrations"
  - "kserve"
  - "mlops"
  - "zenml"
date: "2022-08-04T00:00:00.000Z"
readingTime: 14 Mins Read
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/9be8b31e/652fc9461f361b78b1fcb271_kserve-pipeline.jpg"
seo:
  title: "Deploy your ML models with KServe and ZenML - ZenML Blog"
  description: "How to use ZenML and KServe to deploy serverless ML models in just a few steps."
  canonical: "https://www.zenml.io/blog/deploy-your-ml-models-with-kserve-and-zenml"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/3dffa3e9/652fc9461f361b78b1fcb271_kserve-pipeline.jpg"
  ogTitle: "Deploy your ML models with KServe and ZenML - ZenML Blog"
  ogDescription: "How to use ZenML and KServe to deploy serverless ML models in just a few steps."
---

**Last updated:** November 3, 2022.

The latest [ZenML 0.12.0 release](https://blog.zenml.io/zero-twelve-zero-release/) extends the model deployment story in ZenML by supporting now KServe additionally to already existing MLFlow and Seldon Core, the new integration will allow users to serve, manage and interact with models within the KServe platform, while it also takes care of preparing PyTorch, TensorFlow, and Scikit-Learn models to the right format that Runtimes servers expect.

This post outlines how to use the KServe integration with ZenML. By the end of this post you’ll learn how to:

<ol id=""><li id="">Install the KServe platform on a Kubernetes cluster.</li><li id="">Setup a production-ready MLOps stack with GCP and ZenML with KServe.</li><li id="">Create continuous machine learning pipelines that train, evaluate, deploy and run inference on a PyTorch model.</li></ol>

The content of this post was presented during our weekly community meetup. View the recording [here](https://www.youtube.com/watch?v=nZeBhkN6RZU).

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/8a726d2d/652fc90861a2876115b6602e_0.jpeg" alt="youtube-thumbnail" />
</figure>

## Why KServe?

[KServe](https://github.com/kserve/kserve) (formally KFServing) is a Kubernetes-based model inference platform built for highly-scalable deployment use cases. It provides a standardized inference protocol across ML frameworks while supporting a serverless architecture with autoscaling including Scale to Zero on GPUs. KServe uses a simple and pluggable production serving architecture for production ML serving that includes prediction, pre- and post-processing, monitoring, and explainability. These functionalities and others make KServe one of the most interesting open source tools in the MLOps landscape.

Now let’s see why would you want to use KServe as your ML serving platform:

<ul id=""><li id="">You are looking to deploy your model with an advanced Kubernetes-based model inference platform, built for highly scalable use cases.</li><li id="">You want to handle the lifecycle of the deployed model with no downtime, with automatic scale-up and scale-down capabilities for CPUs and GPUs.</li><li id="">Looking for out-of-the-box model serving runtimes that are easy to use, or easy to deploy models from the well-known frameworks (e.g. TensorFlow, PyTorch, Scikit-learn, XGBoost, etc..)</li><li id="">You want more advanced deployment strategies like A/B testing, canary deployments, ensembles, and <a href="https://kserve.github.io/website/0.9/modelserving/v1beta1/transformer/torchserve_image_transformer/" id="">transformers</a>.</li><li id="">you want to overcome the model deployment Scalability problems. (Read more about KServe Multi-Model Serving or <a href="https://kserve.github.io/website/0.9/modelserving/mms/modelmesh/overview/" id="">ModelMesh</a>.</li></ul>

If you think KServe is the right deployment tool for your MLOps stack or you just want to experiment or learn how to deploy models into a Kubernetes cluster without too much configuration, this post is for you.

## Requirements

In this example, we will use GCP as our cloud provider of choice and provision a GKE Kubernetes cluster and a GCS bucket to store our ML artifacts. (This could also be done in a similar fashion on any other cloud provider.)

This guide expects the following prerequisites:

<ul id=""><li id=""><a href="https://www.python.org/" id="">Python</a> installed (version 3.7-3.9)</li><li id="">Access to a <a href="https://cloud.google.com/" id="">GCP</a> project space</li><li id=""><a href="https://cloud.google.com/sdk/gcloud" id="">gcloud CLI</a> installed on your machine and authenticated</li></ul>

## Setting Up GCP Resources

Before we can deploy our models in KServe we will need to set up all the required resources and permissions on GCP. This is a one-time effort that you will not need to repeat. Feel free to skip and adjust these steps as you see fit.

To start we will create a new GCP project for the express purpose of having all our resources encapsulated into one overarching entity.

Click on the project select box

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/25474daf/652fc90828c1133696409368_gcp-projects-page.png" alt="GCP Projects page" />
</figure>

Create a New Project

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/bd20a958/652fc9098320ec57475de056_gcp-create-project.png" alt="Create new project" />
</figure>

Give the project a name and click on create. The process may take some time. Once that is done, you will need to enable billing for the project so that you can set up all required resources.

### Setting Up GKE

We’ll start off by creating a GKE Standard cluster

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/e2e72974/652fc909aa655260b8a723d2_gcp-create-project.png" alt="Create GCP project" />
</figure>

Optionally, give the cluster a name, otherwise, leave everything as it is since this cluster is only for demo purposes.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/17b22b99/652fc9090a486ba5b69f7316_create-gke-cluster.png" alt="Create GKE standard cluster" />
</figure>

### Cloud Storage

Search cloud storage or use this [link](https://console.cloud.google.com/storage/).

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/f2edaf53/652fc9081f361b78b1fc5581_gcp-create-bucket.png" alt="Create GS bucket" />
</figure>

Once the bucket is created, you can find the storage URI as follows:

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/9303ec11/652fc90959967a6775e737e2_gcp-bucket-uri.png" alt="GS bucket URI" />
</figure>

For the creation of the [ZenML Artifact Store](https://blog.zenml.io/vertex-ai-blog/#zenml-artifact-store) you will need the following data:

<ul id=""><li id="">gsutil URI</li></ul>

### Setting Up Permissions

With all the resources set up, you will now need to set up a service account with all the right permissions. This service account will need to be able to access all the different resources that we have set up so far.

Start by searching for IAM in the search bar or use this link: https://console.cloud.google.com/iam-admin. Here you will need to create a new Service Account.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/fcb00496/652fc909ce6de6122ec560dd_gcp-create-serviceaccount.png" alt="Create Service account" />
</figure>

First off you’ll need to name the service account. Make sure to give it a clear name and description.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/37dbff6e/652fc908e37eab9854582e2b_serviceaccount-details.png" alt="Service account details" />
</figure>

This service account will need to have the role of Storage Admin.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/dfa99a35/652fc909edd487fb7a0f7b85_serviceaccount-roles.png" alt="Service account roles" />
</figure>

Finally, you need to make sure your own account will have the right to run-as this service account. It probably also makes sense to give yourself the right to manage this service account to perform changes later on.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/1fa7cef9/652fc908f3d583e78788b52b_serviceaccount-user-access.png" alt="Service account user access" />
</figure>

Finally, you can now find your new service account in the IAM tab. You’ll need the Principal when creating your ZenML Model Deployer.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/8b25249f/652fc9085f28216b1123cff0_select-serviceaccount.png" alt="Select service account" />
</figure>

We will have to download the service account key; we are going to use this key to grant KServe access to the (ZenML) artifact store.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/94833adf/652fc9095f28216b1123d0de_serviceaccount-keys.png" alt="Service account keys" />
</figure>

We can click on the service account then keys and create a new key and select JSON format.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/d3be7ba6/652fc908aaca78c075588b28_create-serviceaccount-key.png" alt="create a service account key" />
</figure>

## Setting Up KServe and ZenML Stack

Now that we have everything done on the GCP side, we will jump to how we can install KServe on the GKE cluster and then set up our MLOps stack with ZenML CLI

### Installing KServe on GKE

The first thing we need to do is to connect to the GKE cluster. (As mentioned above, this assumes that we have already installed and authenticated with the gcloud CLI.)

We can get the right command to connect to the cluster on the Kubernetes Engine page.

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/1df705f1/652fc908a2e9da205bb8803b_connect-gke-cluster.png" alt="GKE connect" />
</figure>

Now we can copy the command and run it from our terminal

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/3859911c/652fc908544b12cdade47a3a_gke-connect-command.png" alt="GKE connection command" />
</figure>

<ol id=""><li id="">Install Istio:</li></ol>

We need to download [istioctl](https://istio.io/latest/docs/setup/getting-started/#download). Install Istio v1.12.1 (required for the latest KServe version):

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.12.1 &nbsp;sh -
cd istio-1.12.1
export PATH=$PWD/bin:$PATH
# Installing Istio without sidecar injection
istioctl install -y
</code></pre></div>



2.Installing the Knative Serving component:

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
# Install the required custom resources
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.6.0/serving-crds.yaml
# Install the core components of Knative Serving
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.6.0/serving-core.yaml
</code></pre></div>



Install an [Istio networking layer](https://knative.dev/docs/install/installing-istio/):

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
# Install a properly configured Istio
kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml
kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml
# Install the Knative Istio controller
kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml
# Fetch the External IP address or CNAME
kubectl --namespace istio-system get service istio-ingressgateway
</code></pre></div>



Verify the installation:

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
kubectl get pods -n knative-serving

"""
activator-59bff9d7c8-2mgdv &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11h
autoscaler-c574c9455-x7rfn &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3d
controller-59f84c584-mm4pp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3d
domain-mapping-75c659dbc7-hbgnl &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3d
domainmapping-webhook-6d9f5996f9-hcvcb &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3d
net-istio-controller-76bf75d78f-652fm &nbsp; &nbsp;1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11h
net-istio-webhook-9bdb8c6b9-nzf86 &nbsp; &nbsp; &nbsp; &nbsp;1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11h
webhook-756688c869-79pqh &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2d22h
"""
</code></pre></div>

‍

3.Install Cert Manager:

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.yaml
</code></pre></div>

4.Finally, install KServe:

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
# Install KServe
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.9.0/kserve.yaml
# Install KServe Built-in ClusterServingRuntimes
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.9.0/kserve-runtimes.yaml
</code></pre></div>



### Testing KServe Deployment

To test that the installation is functional, you can use this sample KServe deployment:

<ol id=""><li id="">Create a namespace:</li></ol>

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
kubectl create namespace kserve-test
</code></pre></div>



2.Create an InferenceService:

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code">
kubectl apply -n kserve-test -f - &lt;<eof apiversion:="" "serving.kserve.io="" v1beta1"="" kind:="" "inferenceservice"="" metadata:=""  name:="" "sklearn-iris"="" spec:=""  predictor:=""  =""  model:=""  modelformat:="" sklearn=""  storageuri:="" "gs:="" kfserving-examples="" models="" 1.0="" model"="" eof="" <="" code=""></eof></code></pre></div>

`‍`

`3.Check InferenceService status:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
kubectl get inferenceservices sklearn-iris -n kserve-test

"""
NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; URL &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; PREV &nbsp; LATEST &nbsp; PREVROLLEDOUTREVISION &nbsp; LATESTREADYREVISION &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE
sklearn-iris &nbsp; http://sklearn-iris.kserve-test.example.com &nbsp; &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 100 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sklearn-iris-predictor-default-47q2g &nbsp; 7d23h
"""
</code></code></pre></div>

`‍`

`4.Determine the ingress IP and ports:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
kubectl get svc istio-ingressgateway -n istio-system
NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TYPE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CLUSTER-IP &nbsp; &nbsp; &nbsp; EXTERNAL-IP &nbsp; &nbsp; &nbsp;PORT(S) &nbsp; AGE
istio-ingressgateway &nbsp; LoadBalancer &nbsp; 172.21.109.129 &nbsp; 130.211.10.121 &nbsp; ... &nbsp; &nbsp; &nbsp; 17h
</code></code></pre></div>

`‍`

`Extract the HOST and PORT where the model server exposes its prediction API:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
# For GKE clusters, the host is the GKE cluster IP address.
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
# For EKS clusters, the host is the EKS cluster IP hostname.
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')

</code></code></pre></div>

``

`5.Perform inference`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
cat &lt;<eof> "./iris-input.json"
{
 &nbsp;"instances": [
 &nbsp; &nbsp;[6.8, &nbsp;2.8, &nbsp;4.8, &nbsp;1.4],
 &nbsp; &nbsp;[6.0, &nbsp;3.4, &nbsp;4.5, &nbsp;1.6]
 &nbsp;]
}
EOF
</eof></code></code></pre></div>

`‍`

`Use curl to send a test prediction API request to the server:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath='{.status.url}' | cut -d "/" -f 3)
curl -v -H "Host: ${SERVICE_HOSTNAME}" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/sklearn-iris:predict -d @./iris-input.json
</code></code></pre></div>

`‍`

`You should see something like this as the prediction response:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
{"predictions": [1, 1]}
</code></code></pre></div>

### Installing ZenML Integrations

`First, we need to do is install the integration we will be using to run this demo.`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml integration install tensorflow pytorch gcp kserve
</code></code></pre></div>

### ZenML Artifact Store

`The artifact store stores all the artifacts that get passed as inputs and outputs of your pipeline steps. To register our blob storage container:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml artifact-store register gcp_artifact_store --flavor=gcp --path=<gsutil-uri>
</gsutil-uri></code></code></pre></div>

### ZenML Secrets Manager

`The secrets manager is used to securely store all your credentials so ZenML can use them to authenticate with other components like your metadata or artifact store.`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml secrets-manager register local --flavor=local
</code></code></pre></div>

### ZenML Model Deployer

`The Model Deployer is the stack component responsible for serving, managing, and interacting with models. For this demo we are going to register KServe Model Deployer flavor:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml model-deployer register kserve_gke --flavor=kserve \
 &nbsp;--kubernetes_context=gke_kserve-demo_us-east1-b_cluster-1 \ 
 &nbsp;--kubernetes_namespace=zenml-workloads \
 &nbsp;--base_url=$INGRESS_URL \
 &nbsp;--secret=kserve_secret
</code></code></pre></div>

`‍`

### Registering the Stack

`Our stack components are ready to be configured and set as the active stack.`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml stack register local_gcp_kserve_stack -m default -a gcp -o default -d kserve_gke -x local --set
</code></code></pre></div>

## Registering Model Deployer Secret

`Our current stack is using GCS as our Artifact Store which means our trained models will be stored in the GS bucket. This means we need to give KServe the right permissions to be able to retrieve the model artifact. To do that we will create a secret using the service account key we created earlier:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
zenml secrets-manager secret register -s kserve_gs kserve_secret \
 &nbsp; &nbsp;--credentials="@~/kserve-demo.json"
</code></code></pre></div>

`‍`

## Running the Example

`The example uses the digits dataset to train a classifier using both TensorFlow and PyTorch. You can find the full example here. We have two pipelines: one responsible for training and deploying the model and the second one responsible for running inference on the deployed model.`

`The PyTorch Training/Deployment pipeline consists of the following steps:`

<ul id=""><li id=""><code class="language-bash" fs-codehighlight-element="code">importer - Load the MNIST handwritten digits dataset from the TorchVision library</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">train - Train a neural network using the training set. The network is defined in the mnist.py file in the PyTorch folder.</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">evaluate - Evaluate the model using the test set.</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">deployment_trigger - Verify if the newly trained model exceeds the threshold and if so, deploy the model.</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">model_deployer - Deploy the trained model to the KServe model server using the TorchServe runtime.</code></li></ul>

`Let’s take a look at the deployment step and see what is required to deploy a PyTorch model:`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
from zenml.integrations.kserve.services import KServeDeploymentConfig
from zenml.integrations.kserve.steps import (
 &nbsp; &nbsp;KServeDeployerStepConfig,
 &nbsp; &nbsp;TorchServeParameters,
 &nbsp; &nbsp;kserve_model_deployer_step,
)

MODEL_NAME = "mnist-pytorch"

pytorch_model_deployer = kserve_model_deployer_step(
 &nbsp; &nbsp;config=KServeDeployerStepConfig(
 &nbsp; &nbsp; &nbsp; &nbsp;service_config=KServeDeploymentConfig(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;model_name=MODEL_NAME,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;replicas=1,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;predictor="pytorch",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;resources={"requests": {"cpu": "200m", "memory": "500m"}},
 &nbsp; &nbsp; &nbsp; &nbsp;),
 &nbsp; &nbsp; &nbsp; &nbsp;timeout=120,
 &nbsp; &nbsp; &nbsp; &nbsp;torch_serve_parameters=TorchServeParameters(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;model_class="steps/pytorch_steps/mnist.py",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;handler="steps/pytorch_steps/mnist_handler.py",
 &nbsp; &nbsp; &nbsp; &nbsp;),
 &nbsp; &nbsp;)
)
</code></code></pre></div>

`‍`

`Deploying any model using the KServe integration requires some parameters such as the model name, how many replicas we want to have of the pod, which KServe predictor (and this is very important because the platform has already a large list of famous ML frameworks that you can use to serve your models with minimum effort) and finally the resource if we want to limit our deployment to specific limits on CPU and GPU.`

`Because KServe uses TorchServe as the runtime server for deploying PyTorch models we need to provide a model_class path that contains the definition of our neural network architecture and a handler that is responsible for handling the custom pre and post-processing logic. You can read more about how to deploy PyTorch models with TorchServe Runtime Server KServe Pytorch or in the TorchServe Official documentation.`

`The Inference pipeline consists of the following steps:`

<ul id=""><li id=""><code class="language-bash" fs-codehighlight-element="code">pytorch_inference_processor - Load a digits image from a URL (must be 28x28) and converts it to a byte array.</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">prediction_service_loader - Load the prediction service into KServeDeploymentService to perform the inference.</code></li><li id=""><code class="language-bash" fs-codehighlight-element="code">predictor - Perform inference on the image using the built-in predict function of the prediction service.</code></li></ul>

## Running the example

`Wow, we’ve made it past all the setting-up steps, and we’re finally ready to run our code. All we have to do is call our Python function from earlier, sit back and wait!`

<div data-rt-embed-type="true"><pre><code class="language-bash" fs-codehighlight-element="code"><code class="language-bash" fs-codehighlight-element="code">
python run_pytorch.py
</code></code></pre></div>

``

`Once that is done we will see that we have two finished running pipelines, with the result of the prediction from the inference pipeline in addition to detailed information about the endpoint of our served model and how we can use it.`

## Cleaning Up

`Cleanup should be fairly straightforward now, so long as you bundled all of these resources into one separate project. Simply navigate to the Cloud Resource Manager and delete your project:`

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/f341a0a7/652fc9096fc4a4cbf8386553_delete-gcp-project.png" alt="Delete GCP Project" />
</figure>

## Conclusion

`In this tutorial, we learned about KServe and how we can install it on a Kubernetes cluster. We saw how we can setup an MLOps stack with ZenML that uses the KServe Integration to deploy models in a Kubernetes cluster and interact with it using the ZenML CLI.`

`We also saw how the KServe integration makes the experience of deploying a PyTorch model into KServe much easier, as it handles the packaging and preparing of the different resources that are required by TorchServe.`

`We are working on making the deployment story more customizable by allowing the user to write their own custom logic to be executed before and after the model gets deployed. This will be a great feature for not only serving the model but that will allow for custom code to be deployed alongside the serving tools.`

`This demo was presented at our community meetup. Check out the recording here if you’d like to follow a live walkthrough of the steps listed above.`

`If you have any questions or feedback regarding this tutorial, join our weekly community hour.`

`If you want to know more about ZenML or see more examples, check out our docs and examples or join our Slack.`

`‍`