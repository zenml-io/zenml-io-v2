---
title: "Supercharge Open Source ML Workflows with ZenML And Skypilot"
slug: "supercharge-open-source-ml-workflows-with-zenml-and-skypilot"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "66d1eb96fae93438864b6b25"
  exportedAt: "2026-02-11T10:23:34.071Z"
  source: "live"
  lastPublished: "2024-09-02T12:17:16.743Z"
  lastUpdated: "2024-08-30T16:13:33.252Z"
  createdOn: "2024-08-30T15:56:06.934Z"
author: "hamza-tahir"
category: "tutorials"
tags:
  - "mlops"
  - "zenml"
  - "skypilot"
date: "2024-08-30T00:00:00.000Z"
readingTime: 5 mins
mainImage:
  url: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/5c4b3ab8/66d1eb64268497ccabeea28c_zenml-skypilot.png"
seo:
  title: "Supercharge Open Source ML Workflows with ZenML And Skypilot - ZenML Blog"
  description: "The combination of ZenML and SkyPilot offers a robust solution for managing ML workflows."
  canonical: "https://www.zenml.io/blog/supercharge-open-source-ml-workflows-with-zenml-and-skypilot"
  ogImage: "https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/5c4b3ab8/66d1eb64268497ccabeea28c_zenml-skypilot.png"
  ogTitle: "Supercharge Open Source ML Workflows with ZenML And Skypilot - ZenML Blog"
  ogDescription: "The combination of ZenML and SkyPilot offers a robust solution for managing ML workflows."
---

Whether you're an ML engineer focusing on infrastructure or a data scientist diving into model development, the combination of ZenML and [SkyPilot](https://skypilot.readthedocs.io/) offers a robust solution for managing ML workflows. This [integration](https://docs.zenml.io/how-to/popular-integrations/skypilot) bridges the gap between rapid experimentation and scalable cloud execution.

Best part? Both tools are free and open source!

## Why ZenML + SkyPilot?

SkyPilot brings its own set of powerful capabilities to the world of MLOps/LLMOps. As an open-source orchestration framework, SkyPilot excels in cloud-agnostic workloads, allowing users to run AI jobs on any cloud with minimal code changes. It offers intelligent cloud selection based on cost and availability, automatic spot instance handling for cost savings, and efficient management of cloud storage. SkyPilot's ability to easily launch, scale, and manage cloud resources makes it an ideal complement to ZenML's MLOps functionalities. 

ZenML is an open source MLOps framework, that also abstracts away infrastructure complexity for cloud-agnostic ML workloads, but has less of a focus on the actual orchestration itself. Rather, it focuses on observability, reproducibility, and emphasizes the production stage of ML development. 

Therefore, both products have clear synergies. Here are the advantages of using both together:

<ol id=""><li id=""><strong id="">Python-Centric Workflows</strong>: Define pipelines in Python, even within notebooks, instead of YAML.</li><li id=""><strong id="">Abstracted Orchestration</strong>: Hide infrastructure details, focusing on ML logic.</li><li id=""><strong id="">Flexible Execution</strong>: Switch between local and cloud runs with minimal changes.</li><li id=""><strong id="">Comprehensive Tracking</strong>: Automatically version code, metadata, and data.</li><li id=""><strong id="">Automated Containerization</strong>: Simplify dependency management and reproducibility.</li></ol>

## Implementation Example

A good example to see the difference from good-old plain Skypilot, would be to take the [quickstart training example](https://skypilot.readthedocs.io/en/latest/getting-started/tutorial.html), and see how it would work with ZenML.

First, install the required packages:

<div data-rt-embed-type="true"><pre><code>pip install "zenml[server]" "zenml[skypilot]" torch transformers datasets
zenml integration install huggingface -y
# run zenml login on a deployed server
zenml login</code></pre></div>

Now, start writing your workflows. Here's a multi-step pipeline for fine-tuning a BERT model on the [GLUE MRPC dataset](https://huggingface.co/datasets/nyu-mll/glue):

<div data-rt-embed-type="true"><pre><code>from zenml import pipeline, step
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, BertForSequenceClassification
from typing import Tuple
import datasets

@step(enable_cache=False)
def load_data() -&gt; datasets.DatasetDict:
 &nbsp; &nbsp;dataset = datasets.load_dataset("glue", "mrpc")
 &nbsp; &nbsp;return dataset

@step
def preprocess_data(dataset: datasets.DatasetDict) -&gt; Tuple[datasets.Dataset, datasets.Dataset]:
 &nbsp; &nbsp;tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
 &nbsp; &nbsp;
 &nbsp; &nbsp;def tokenize_function(examples):
 &nbsp; &nbsp; &nbsp; &nbsp;return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length")
 &nbsp; &nbsp;
 &nbsp; &nbsp;tokenized_datasets = dataset.map(tokenize_function, batched=True)
 &nbsp; &nbsp;return tokenized_datasets["train"], tokenized_datasets["validation"]

@step
def train_model(train_dataset: datasets.Dataset, eval_dataset: datasets.Dataset) -&gt; BertForSequenceClassification:
 &nbsp; &nbsp;model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
 &nbsp; &nbsp;
 &nbsp; &nbsp;training_args = TrainingArguments(
 &nbsp; &nbsp; &nbsp; &nbsp;output_dir="./results",
 &nbsp; &nbsp; &nbsp; &nbsp;num_train_epochs=3,
 &nbsp; &nbsp; &nbsp; &nbsp;per_device_train_batch_size=16,
 &nbsp; &nbsp; &nbsp; &nbsp;per_device_eval_batch_size=16,
 &nbsp; &nbsp; &nbsp; &nbsp;warmup_steps=500,
 &nbsp; &nbsp; &nbsp; &nbsp;weight_decay=0.01,
 &nbsp; &nbsp; &nbsp; &nbsp;logging_dir='./logs',
 &nbsp; &nbsp;)
 &nbsp; &nbsp;
 &nbsp; &nbsp;trainer = Trainer(
 &nbsp; &nbsp; &nbsp; &nbsp;model=model,
 &nbsp; &nbsp; &nbsp; &nbsp;args=training_args,
 &nbsp; &nbsp; &nbsp; &nbsp;train_dataset=train_dataset,
 &nbsp; &nbsp; &nbsp; &nbsp;eval_dataset=eval_dataset
 &nbsp; &nbsp;)
 &nbsp; &nbsp;
 &nbsp; &nbsp;trainer.train()
 &nbsp; &nbsp;return model

@step
def evaluate_model(model: BertForSequenceClassification, eval_dataset: datasets.Dataset) -&gt; dict:
 &nbsp; &nbsp;trainer = Trainer(model=model)
 &nbsp; &nbsp;results = trainer.evaluate(eval_dataset)
 &nbsp; &nbsp;return results

@pipeline
def glue_fine_tuning_pipeline():
 &nbsp; &nbsp;dataset = load_data()
 &nbsp; &nbsp;train_dataset, eval_dataset = preprocess_data(dataset)
 &nbsp; &nbsp;model = train_model(train_dataset, eval_dataset)
 &nbsp; &nbsp;results = evaluate_model(model, eval_dataset)

if __name__ == "__main__":
 &nbsp; &nbsp;glue_fine_tuning_pipeline()</code></pre></div>

## Running the Pipeline

1. Local Execution:

<div data-rt-embed-type="true"><pre><code>glue_fine_tuning_pipeline()</code></pre></div>

2. SkyPilot Execution:

<div data-rt-embed-type="true"><pre><code>from zenml.config import DockerSettings
from zenml.integrations.skypilot.flavors import SkypilotOrchestratorSettings

docker_settings = DockerSettings(
		required_integrations=["huggingface"],
 &nbsp; &nbsp;requirements=["torch", "transformers", "datasets"],
)
skypilot_settings = SkypilotOrchestratorSettings(
 &nbsp; &nbsp;instance_type="p3.2xlarge",
 &nbsp; &nbsp;use_spot=True,
 &nbsp; &nbsp;region="us-west-2"
)

glue_fine_tuning_pipeline.with_options(
 &nbsp; &nbsp;config_path="config.yaml",
 &nbsp; &nbsp;settings={
 &nbsp; &nbsp; &nbsp; &nbsp;"docker": docker_settings,
 &nbsp; &nbsp; &nbsp; &nbsp;"orchestrator.vm_aws": skypilot_settings
 &nbsp; &nbsp;}
)()</code></pre></div>

This demonstrates the ease of transitioning from local to cloud execution without altering the core pipeline logic. In both cases, this is how it will show up on the dashboard:

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/37cc2e56/66d1eb96fae93438864b6a7a_66d1ea95d96236f53db1c500_pipeline1.png" alt="ML pipeline diagram with code for training model step" />
</figure>

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/d79db552/66d1eb96fae93438864b6a88_66d1eaa41ac762c654d78869_pipeline2.png" alt="ML pipeline diagram with details of preprocess_data output" />
</figure>

<figure>
  <img src="https://pub-d0f853843b954aadbcd60eaff1d9c6e2.r2.dev/webflow/64a817a2e7e2208272d1ce30/164d7ff4/66d1eb96fae93438864b6a85_66d1eabc4427e9c56d35f759_pipeline3.png" alt="ML pipeline diagram with run details and overview" />
</figure>

Notice how much more shared, collaborative, and observable this run is, vs. having it run ad-hoc. This is the power of having a shared MLOps framework.

## MLOps Platform Perspective: Enhancing Team Productivity at Scale

Integrating ZenML with SkyPilot offers significant advantages for scaling ML operations across larger data science organizations:

<ol id=""><li id=""><strong id="">Resource Optimization</strong>: Centralized tracking of cloud resource usage across all ML projects enables better allocation and cost management.</li><li id=""><strong id="">Standardization</strong>: Enforce consistent workflows and best practices across diverse teams and projects.</li><li id=""><strong id="">Collaboration</strong>: Improved visibility into model development processes and results fosters knowledge sharing and reduces redundant work.</li><li id=""><strong id="">Unified Interface</strong>: A single platform for managing ML experiments, models, and deployments streamlines operations.</li><li id=""><strong id="">Scalability</strong>: Seamlessly transition from experimentation to production-scale workflows without changing tools.</li></ol>

Instead of fragmented tooling and ad-hoc scripts, ZenML provides a centralized interface that tracks experiments, models, and metrics. This comprehensive view enables data science leaders to make informed decisions about resource allocation and project priorities, while the underlying SkyPilot integration ensures efficient use of cloud resources.

## Key Advantages

<ol id=""><li id=""><strong id="">Clear Separation of Concerns</strong>: Isolated steps improve maintainability and reusability.</li><li id=""><strong id="">Flexible Resource Configuration</strong>: Adjust cloud resources via simple ZenML settings.</li><li id=""><strong id="">Version Control</strong>: Automatic tracking of data, code, and model versions.</li><li id=""><strong id="">Cost Optimization</strong>: Leverage SkyPilot's spot instance and multi-region pricing features.</li><li id=""><strong id="">Reproducibility</strong>: Containerized environments ensure consistent execution across different environments.</li></ol>

## Conclusion

The ZenML + SkyPilot integration offers a powerful solution for ML teams, from individual contributors to large-scale data science operations. It combines the simplicity of ZenML's pipeline abstraction with the efficiency of SkyPilot's cloud orchestration. This approach maintains agility throughout the ML lifecycle while providing the structure necessary for scaling ML operations.

By abstracting infrastructure complexities, this integration allows data scientists and ML engineers to focus on model development and experimentation. Simultaneously, it gives MLOps teams the tools to standardize practices, optimize resources, and foster collaboration across the organization. The seamless transition between local and cloud environments, coupled with comprehensive versioning and tracking, makes this an invaluable asset for modern ML workflows in organizations of any size.

Try out ZenML with Skypilot today with the [starter guide](https://docs.zenml.io/user-guide/starter-guide), and let us know on [Slack](https://zenml.io/slack) how it went!