---
title: "End-to-End Foundation Models for Self-Driving Vehicles at Scale"
slug: "end-to-end-foundation-models-for-self-driving-vehicles-at-scale"
draft: false
webflow:
  siteId: "64a817a2e7e2208272d1ce30"
  itemId: "6930555e6b14baff59921b2c"
  exportedAt: "2026-02-11T13:30:32.135Z"
  source: "live"
  lastPublished: "2025-12-23T12:21:17.478Z"
  lastUpdated: "2025-12-18T17:31:00.473Z"
  createdOn: "2025-12-03T15:21:02.827Z"
llmopsTags:
  - "fine-tuning"
  - "few-shot"
  - "model-optimization"
  - "latency-optimization"
  - "human-in-the-loop"
  - "evals"
  - "pytorch"
  - "monitoring"
  - "scaling"
  - "orchestration"
industryTags: "automotive"
company: "Wayve"
summary: "Wayve is developing self-driving technology that works across multiple vehicle types and global markets by leveraging end-to-end foundation models trained on driving data rather than traditional rule-based systems. The company moved away from intermediate representations like object detection to a more holistic approach where a single neural network learns to drive from examples, similar to how large language models learn language. This architecture enabled rapid global expansion from primarily driving in London to operating across 500 cities in Japan, Europe, the UK, and the US within a year. The system uses foundation models for multiple tasks including driving, simulation, scenario classification, and even natural language explanations of driving decisions, with all components compressed into a single 75-watt model deployable in production vehicles."
link: "https://www.youtube.com/watch?v=W-odbzoV6TM"
year: 2025
seo:
  title: "Wayve: End-to-End Foundation Models for Self-Driving Vehicles at Scale - ZenML LLMOps Database"
  description: "Wayve is developing self-driving technology that works across multiple vehicle types and global markets by leveraging end-to-end foundation models trained on driving data rather than traditional rule-based systems. The company moved away from intermediate representations like object detection to a more holistic approach where a single neural network learns to drive from examples, similar to how large language models learn language. This architecture enabled rapid global expansion from primarily driving in London to operating across 500 cities in Japan, Europe, the UK, and the US within a year. The system uses foundation models for multiple tasks including driving, simulation, scenario classification, and even natural language explanations of driving decisions, with all components compressed into a single 75-watt model deployable in production vehicles."
  canonical: "https://www.zenml.io/llmops-database/end-to-end-foundation-models-for-self-driving-vehicles-at-scale"
  ogTitle: "Wayve: End-to-End Foundation Models for Self-Driving Vehicles at Scale - ZenML LLMOps Database"
  ogDescription: "Wayve is developing self-driving technology that works across multiple vehicle types and global markets by leveraging end-to-end foundation models trained on driving data rather than traditional rule-based systems. The company moved away from intermediate representations like object detection to a more holistic approach where a single neural network learns to drive from examples, similar to how large language models learn language. This architecture enabled rapid global expansion from primarily driving in London to operating across 500 cities in Japan, Europe, the UK, and the US within a year. The system uses foundation models for multiple tasks including driving, simulation, scenario classification, and even natural language explanations of driving decisions, with all components compressed into a single 75-watt model deployable in production vehicles."
---

## Overview

Wayve's presentation provides a comprehensive look at how foundation models and LLMOps principles are being applied to physical AI, specifically in the autonomous driving domain. The speaker, an engineer at Wayve, describes their journey from traditional rule-based autonomous driving systems to an end-to-end learning approach that mirrors the evolution seen in natural language processing with large language models. The case study is particularly relevant to LLMOps because it demonstrates how production AI systems must handle real-world deployment challenges including multi-sensor inputs, cross-geographical generalization, continuous learning from operational data, and extreme computational constraints.

The company's core innovation lies in abandoning the traditional autonomous driving stack that relies on intermediate representations (object detection, tracking, behavior prediction) in favor of a single foundation model that learns directly from sensor inputs to driving actions. This architectural decision enabled them to scale from operating primarily in London to 500 cities across multiple continents within approximately one year, representing a significant achievement in production AI deployment.

## Technical Architecture and Model Design

Wayve's system architecture centers on what they call a "world model" or foundation model that serves as the core understanding engine for their autonomous driving system. Unlike traditional approaches that decompose the driving problem into discrete perception, prediction, and planning modules, Wayve's end-to-end approach trains a single neural network to map sensor inputs directly to driving trajectories and control decisions.

The foundation model is trained on diverse data sources beyond just driving scenarios. The speaker emphasizes that this model develops an understanding of geometry, kinematics, and spatial reasoning similar to how humans develop intuitive physics from childhood experiences. This holistic world understanding doesn't come just from driving data but from multiple related tasks, enabling the model to develop generalizable representations that transfer across different contexts.

A crucial aspect of their production deployment is model compression. The foundation model itself may be quite large and not suitable for deployment in vehicles, so Wayve has developed processes to compress and optimize the model to run within a 75-watt power budget—a significant constraint for edge deployment in automotive contexts. The final deployed model must handle all driving tasks including trajectory generation, safety verification, and auxiliary signal generation for integration with vehicle systems, all while running in real-time on limited compute resources.

The architecture supports multiple output modalities from the same underlying foundation model. Beyond driving actions, the model can perform next-frame prediction for simulation, generate natural language explanations of driving decisions, and classify driving scenarios for data curation purposes. This multi-task capability is central to their LLMOps approach, as it enables the same model infrastructure to support both production driving and the development toolchain.

## Training Data and Learning Paradigm

The data requirements for Wayve's system are substantial and present significant LLMOps challenges. The speaker mentions that one million hours of sensor data, if uncompressed, would amount to an exabyte of storage—far exceeding typical training data volumes in other AI domains. This necessitates sophisticated data management, compression, and curation strategies.

Wayve employs multiple data acquisition strategies including operating their own test fleet, partnering with OEMs (original equipment manufacturers) for data sharing, and potentially purchasing third-party data including dashcam footage. The heterogeneity of data sources presents both opportunities and challenges. For example, dashcam data may capture more near-miss incidents and edge cases than carefully curated test drives, but it comes with quality issues like single-camera perspectives, shaky footage, and lack of precise vehicle motion data. Wayve has developed models specifically for reconstructing vehicle dynamics and motion from imperfect sensor data to make such datasets usable.

The learning paradigm is explicitly compared to how large language models acquire language understanding. Just as LLMs pointed at Wikipedia learn multiple languages without explicit programming for each one, Wayve's driving model learns to handle different traffic patterns, road configurations, and regulatory environments through exposure to diverse data. The speaker emphasizes that the model is not programmed with rules like "drive on the left" or "stop at red lights"—instead, it learns these behaviors by imitating human drivers across many examples.

A key innovation is imitation learning from human demonstrations. The model can be trained on data from human drivers navigating various scenarios, learning not just the specific maneuvers but the underlying patterns that generalize to new situations. This approach enables zero-shot transfer to new geographic regions—the speaker describes taking a model trained in the UK and deploying it in Japan with supervision, where it immediately begins adapting to local driving conventions like triangular stop signs that differ from UK signage.

## Data Curation and the AI-for-AI Flywheel

One of the most sophisticated aspects of Wayve's LLMOps infrastructure is what the speaker describes as "using AI to build AI." The sheer volume of driving data makes manual curation infeasible, so Wayve has developed AI-powered tooling throughout their development pipeline.

Their foundation model is used for scenario classification and data indexing, helping engineers identify relevant training examples from massive datasets. For instance, if they need to ensure the model has seen all 2,000 traffic signs present in Europe, they use models to search through collected data and verify coverage. This data curriculum approach ensures systematic exposure to all relevant scenarios before deployment.

The generative capabilities of the foundation model play a crucial role in creating synthetic training and test data. By treating the model as a generative system capable of next-frame prediction, Wayve can simulate driving scenarios that extend beyond what was captured in real data. The speaker demonstrates this capability extensively—the model can generate realistic multi-second video sequences showing what would happen if the vehicle took different actions than those in the original recording.

Particularly interesting is their use of "bad dreams"—deliberately generating out-of-distribution scenarios where the vehicle makes mistakes or encounters dangerous situations. By conditioning the model to generate scenarios where it crosses lane boundaries, follows another vehicle too closely, or encounters oncoming traffic, they can synthesize training data for edge cases that would be rare or impossible to collect safely in the real world. The speaker emphasizes that these generated scenarios maintain physical realism despite being extrapolations from real data, with the model generating coherent 10-frame-per-second sequences that don't degrade even when fed back into the model recursively.

This creates a virtuous cycle: better models enable better data selection and curation, which in turn produces better training datasets, leading to improved models. The speaker explicitly frames this as a "snowball" or "virtual cycle" where the system becomes progressively more intelligent about its own development process.

## Multimodal Capabilities and Explainability

A distinctive feature of Wayve's production system is its integration of language model capabilities alongside driving functionality. The speaker describes training "a language model together with our driving model" resulting in "basically one model" that can both drive and explain its actions in natural language.

This multimodal architecture provides several benefits for production deployment. The model can reference specific sections of the UK driving code when explaining why it's slowing down—for example, noting the requirement to maintain four seconds of following distance behind vulnerable road users in rainy conditions. This explainability is crucial for debugging, safety validation, and potentially for meeting regulatory requirements as autonomous systems move toward broader deployment.

The integration of language capabilities also suggests that Wayve is leveraging techniques from the LLM domain such as joint embedding spaces and multimodal training objectives. By training driving and language understanding together, the model can develop richer semantic representations that connect visual driving scenarios to conceptual understanding expressed in language.

The speaker emphasizes that concepts emerge in their driving model just as they do in language models—the system isn't explicitly programmed with concepts like "lane," "car," or "pedestrian," but instead develops these abstractions naturally through training on examples. This emergent behavior is presented as a key advantage enabling generalization across diverse environments and scenarios.

## Deployment and Production Constraints

The production deployment of Wayve's technology involves significant LLMOps challenges related to hardware constraints, sensor heterogeneity, and automotive reliability requirements. The final deployed system must operate within a 75-watt power budget, necessitating aggressive model optimization and compression from the larger foundation model used in training and development.

Wayve's approach is designed to work across multiple vehicle types and sensor configurations rather than requiring bespoke development for each vehicle-sensor combination. Traditional autonomous driving systems often involve extensive ISP (image signal processor) tuning and assume specific sensor layouts. Wayve's end-to-end learning approach is intentionally less sensitive to these details, mimicking how human drivers can adapt to different vehicles and viewing angles.

The speaker describes working with multiple OEMs who have their own preferred sensor suppliers and integration timelines. Rather than insisting on specific sensor specifications, Wayve's technology is designed to adapt to whatever sensors are available in a given vehicle. This flexibility is crucial for achieving scale in the automotive market where manufacturers have established supply chains and integration processes.

The system must also meet automotive-grade reliability standards—a significantly higher bar than typical software deployments. The speaker mentions the need to "validate this model" and ensure it has comprehensive coverage of relevant scenarios before deployment. This involves systematic testing against data curricula, simulation validation, and supervised on-road testing with safety drivers before any unsupervised operation.

Wayve supports multiple levels of autonomy in production, from "hands-off" systems where the driver keeps eyes on the road but doesn't need to touch the steering wheel, to "eyes-off" systems where the driver can engage in other activities. The speaker personally advocates for the eyes-off capability, calculating that the average commuter could reclaim approximately one year of life over a 30-year career if their commute time became productive.

## Global Scaling and Zero-Shot Transfer

One of the most impressive aspects of Wayve's production deployment is the rapid geographic scaling achieved through their learning-based approach. The speaker describes reaching an "inflection point" about a year before the presentation (approximately 2024), after which the system's ability to generalize accelerated dramatically. From primarily operating in London, they expanded to Japan, across Europe, the UK, and the US, eventually testing in over 500 cities.

This scalability stems directly from their architectural decision to avoid high-definition maps and rule-based systems. Traditional autonomous driving approaches often rely on precisely mapped environments where the vehicle can localize itself to centimeter accuracy against pre-built 3D maps. These maps are expensive to create and maintain, and become outdated when roads are modified or temporarily altered by construction.

By learning to drive from sensor data alone without assuming prior maps, Wayve's system can operate in previously unseen locations immediately—a true zero-shot capability. The speaker describes OEM partners deliberately testing this by refusing to disclose test routes in advance, instead directing the vehicle to arbitrary locations and expecting it to navigate successfully. This is only possible because the system hasn't memorized specific routes but has learned generalizable driving skills.

The first deployment in Japan in March (presumably 2024 or early 2025) illustrates both the capabilities and limitations of zero-shot transfer. The model trained in the UK could immediately drive in Japan since both countries drive on the left side of the road and share many road infrastructure conventions. However, certain country-specific elements like triangular stop signs required local learning. This was accomplished through both data partnerships providing Japanese driving footage for training and supervised on-road operation that generated new training data specific to the Japanese context.

The speaker emphasizes that even minor variations between environments are handled gracefully because the model learns from examples rather than following programmatic rules. Two identical vehicles will have slightly different tire inflation, steering bias, and other physical characteristics—the kind of variation that would require explicit handling in traditional control systems but that the learned model adapts to naturally through its training on diverse examples.

## Development Toolchain and Infrastructure

The speaker places significant emphasis on the development toolchain as an enabler of rapid iteration and scaling. The overall development flow involves data collection from various sources (own fleet, partner fleets, purchased data), model training, evaluation in simulation, on-road validation, and then feeding learnings back into the next iteration of data collection and training.

The toolchain includes several sophisticated components:

- **Simulation infrastructure** powered by the generative capabilities of the foundation model itself, enabling realistic scenario generation and counterfactual analysis
- **Data indexing and search** using AI models to find relevant scenarios within massive datasets
- **Scenario classification** to organize data according to relevant categories for training curriculum design
- **Localization and motion estimation** models to process lower-quality data sources like dashcam footage that lack precise motion ground truth
- **Validation tools** to ensure model coverage of required scenarios before deployment

The speaker emphasizes that "you need AI to develop AI" and that there's "a limit to how much AI you can develop purely with software." This reflects a broader LLMOps principle that production AI systems require AI-powered infrastructure for their own development and operations, not just traditional software engineering.

The fact that Wayve can deploy "one model that runs across countries across cars" is presented as a major advantage over traditional approaches where each new market or vehicle variant requires substantial re-engineering. This generalization comes from the learning-based approach but also from the sophisticated toolchain that enables systematic validation and ensures the single model has learned everything necessary for diverse deployment contexts.

## Critical Assessment and Challenges

While the presentation showcases impressive capabilities, it's important to note that this is a talk from Wayve itself and likely emphasizes successes while downplaying challenges. Several aspects warrant critical consideration:

The speaker mentions that operations are conducted with safety drivers and under safety regulations, indicating that fully unsupervised operation has not been achieved. The distinction between hands-off and eyes-off automation levels suggests they are targeting SAE Level 3 or higher autonomy, which remains a significant technical and regulatory challenge industry-wide.

The claim of operating in 500 cities should be understood in context—this likely means tested operation rather than continuous commercial deployment. The speaker describes testing "uninterrupted drive from central London to Birmingham" as noteworthy, suggesting that reliable long-distance operation is still being validated rather than routinely deployed.

The data volumes required are enormous, and the speaker acknowledges the need for compression and smart data selection. While they've developed AI tools to help with curation, the fundamental challenge of collecting and managing petabyte-to-exabyte scale multimodal data remains substantial. The reliance on data partnerships and purchased data also introduces questions about data quality control and consistency across sources.

The generative simulation approach, while innovative, must be validated to ensure that the synthetic scenarios are truly realistic and safety-relevant. The speaker notes "it has to be correct, it has to be useful" but doesn't detail how they validate that generated scenarios maintain physical realism and represent meaningful edge cases rather than artifacts of the generative model.

The end-to-end learning approach may offer better generalization than rule-based systems, but it also presents challenges for interpretability and safety verification. While the integration of language models for explanation is a step toward interpretability, whether these explanations truly reflect the model's decision process or are post-hoc rationalizations remains an open question.

## Production AI and LLMOps Lessons

This case study illustrates several important principles for deploying foundation models in production, particularly in physical AI applications:

**End-to-end learning can enable generalization** that rule-based systems struggle to achieve, but requires massive amounts of diverse data and sophisticated curation infrastructure to realize that potential in practice.

**Model compression and optimization** are critical for edge deployment in resource-constrained environments. The gap between foundation models used in training and the 75-watt production model represents significant MLOps engineering effort.

**Multi-task learning from shared representations** can be leveraged to build not just the production model but the entire development toolchain, with the same foundation model supporting driving, simulation, explanation, and data curation tasks.

**Continuous learning from operational deployment** is essential in physical AI where the environment is non-stationary and presents endless variations. The flywheel between deployment, data collection, training, and improved deployment is central to the approach.

**Generative models can augment training data** with synthetic scenarios, particularly for rare edge cases that would be dangerous or impossible to collect in practice. This addresses a fundamental challenge in safety-critical applications.

**Zero-shot transfer capabilities** from foundation models can dramatically accelerate scaling to new contexts, though local fine-tuning and validation remain necessary for production deployment in novel environments.

The Wayve case study represents an interesting convergence of techniques from the LLM and computer vision communities being applied to physical AI with all the additional constraints that entails. While the presentation naturally emphasizes successes, the overall architecture and approach offer valuable lessons for deploying foundation models in production systems beyond traditional software applications.